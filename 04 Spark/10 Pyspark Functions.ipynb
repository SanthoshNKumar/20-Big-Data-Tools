{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Win10.eu2d4xqpkiwuzc50gvspb3qxbb.bx.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkByExamples.com</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1da1ed37550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Create Spark Session\n",
    "\n",
    "# Testing pyspark Intallation\n",
    "import findspark\n",
    "findspark.init('C:\\Spark')\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark configuration\n",
    "from pyspark import SparkConf\n",
    "conf = SparkConf().setAppName(\"Spark Demo\").setMaster(\"local\")\n",
    "\n",
    "# pass Spark configuration on SparkContext\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SaprkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create SparkContext\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"My First Spark Application\")\n",
    "\n",
    "# Create Spark RDD using SparkContext parallelize\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "\n",
    "#Create RDD from external Data source\n",
    "rdd2 = spark.sparkContext.textFile(\"/path/textFile.txt\")\n",
    "\n",
    "#Reads entire file into a RDD as single record.\n",
    "rdd3 = spark.sparkContext.wholeTextFiles(\"/path/textFile.txt\")\n",
    "\n",
    "# Creates empty RDD with no partition    \n",
    "rdd = spark.sparkContext.emptyRDD \n",
    "# rddString = spark.sparkContext.emptyRDD[String]\n",
    "\n",
    "#Create empty RDD with partition\n",
    "rdd2 = spark.sparkContext.parallelize([],10) #This creates 10 partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Get Active Spark session\n",
    "spark.getActiveSession()\n",
    "\n",
    "# Get Spark Version\n",
    "spark.version\n",
    "\n",
    "# Get app name\n",
    "spark.sparkContext.appName\n",
    "\n",
    "# get master \n",
    "spark.sparkContext.master\n",
    "\n",
    "# Paralleize the task using Sparkcontext\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# create Empty RDD\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Broadcast varibles\n",
    "broadcastVar = spark.sparkContext.broadcast([0, 1, 2, 3])\n",
    "broadcastVar.value\n",
    "\n",
    "# Accumulator varibles\n",
    "accum = spark.sparkContext.longAccumulator(\"SumAccumulator\")\n",
    "spark.sparkContext.parallelize([1, 2, 3]).foreach(lambda x: accum.add(x))\n",
    "\n",
    "# Create DataFrame\n",
    "df=spark.createDataFrame(data,columns)\n",
    "\n",
    "# Create Empty DataFrame\n",
    "df=spark.emptyDataFrame()\n",
    "\n",
    "# Create Dataset\n",
    "df=spark.createDataset()\n",
    "\n",
    "# Create Empty DataSet\n",
    "df=spark.emptyDataFrame()\n",
    "\n",
    "# Stop Sparkcontext\n",
    "spark.stop()\n",
    "\n",
    "# Returns a SparkContext\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert RDD to dataframe\n",
    "\n",
    "# Create RDD\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Using toDF() function\n",
    "df = rdd.toDF()\n",
    "\n",
    "# Create Empty RDD\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Create Dataframe from Empty dataFrame\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([StructField('firstname', StringType(), True),\n",
    "                     StructField('middlename', StringType(), True),\n",
    "                     StructField('lastname', StringType(), True)])\n",
    "\n",
    "# Create empty DataFrame from empty RDD\n",
    "df = spark.createDataFrame(emptyRDD,schema)\n",
    "\n",
    "# Convert Empty RDD to DataFrame\n",
    "df1 = emptyRDD.toDF(schema)\n",
    "\n",
    "# Create Empty DataFrame with Schema.\n",
    "df2 = spark.createDataFrame([], schema)\n",
    "\n",
    "# Create Empty DataFrame without Schema (no columns)\n",
    "df3 = spark.createDataFrame([], StructType([]))\n",
    "\n",
    "rdd3.getNumPartitions()\n",
    "\n",
    "rdd3_coalesce = rdd3.coalesce(1)\n",
    "rdd3_coalesce.getNumPartitions()\n",
    "\n",
    "# RDD Cache\n",
    "cachedRdd = rdd.cache()\n",
    "\n",
    "dfPersist = df.persist()\n",
    "\n",
    "# RDD Persist\n",
    "import pyspark\n",
    "dfPersist = rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "dfPersist.show(false)\n",
    "    \n",
    "# RDD Unpersist\n",
    "rddPersist2 = rddPersist.unpersist()\n",
    "\n",
    "############################################## Transformations ################################################################\n",
    "\n",
    "rdd = sc.textFile(\"PATH/blogtexts\")\n",
    "rdd.take(5)\n",
    "\n",
    "# Sample\n",
    "rdd3_sampled = rdd3.sample(False, 0.4, 42)\n",
    "\n",
    "rdd3_sampled.collect()\n",
    "\n",
    "# flapmap\n",
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "# map\n",
    "rdd3 = rdd2.map(lambda x: (x,1))\n",
    "\n",
    "rdd6 = rdd5.map(lambda x: (x[1],x[0])).sortByKey()\n",
    "\n",
    "# reduceByKey\n",
    "rdd5 = rdd4.reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "\n",
    "# Transformation: filter\n",
    "\n",
    "# Filter\n",
    "stopwords = ['is','am','are','the','for','a']\n",
    "rdd3 = rdd2.filter(lambda x: x not in stopwords)\n",
    "\n",
    "# take\n",
    "rdd3.take(10)\n",
    "\n",
    "\n",
    "# Set Theory / Relational Transformation\n",
    "        # Transformation: union\n",
    "        # Transformation: join\n",
    "\n",
    "        # Transformation: distinct\n",
    "rdd3_distinct = rdd3.distinct()\n",
    "len(rdd3_distinct.collect())\n",
    "\n",
    "################################################# Actions ###################################################################\n",
    "\n",
    "# collect\n",
    "data = rdd6.collect()\n",
    "for f in data:\n",
    "    print(\"Key:\"+ str(f[0]) +\", Value:\"+f[1])\n",
    "\n",
    "# Reduce\n",
    "num_rdd = sc.parallelize(range(1,1000))\n",
    "num_rdd.reduce(lambda x,y: x+y)\n",
    "\n",
    "# Mathematical / Statistical Actions\n",
    "num_rdd.max(),num_rdd.min(), num_rdd.sum(),num_rdd.variance(),num_rdd.stdev() \n",
    "\n",
    "# first\n",
    "firstRec = rdd6.first()\n",
    "print(\"First Record : \"+str(firstRec[0]) + \",\"+ firstRec[1])\n",
    "\n",
    "# max\n",
    "datMax = rdd6.max()\n",
    "\n",
    "# take\n",
    "data3 = rdd6.take(3)\n",
    "for f in data3:\n",
    "    print(\"data3 Key:\"+ str(f[0]) +\", Value:\"+f[1])\n",
    "\n",
    "# saveAsTextFile\n",
    "rdd6.saveAsTextFile(\"/tmp/wordCount\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Write DataFrmes from ExternalFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe from Read external files\n",
    "\n",
    "\n",
    "################################################## Read ####################################################################\n",
    "\n",
    "# Creating df from csv file\n",
    "df2 = spark.read.option(\"header\",True).csv(\"/src/resources/file.csv\")\n",
    "\n",
    "# Creating df from text (TXT) file\n",
    "df2 = spark.read.option(\"header\",True).text(\"/src/resources/file.txt\")\n",
    "\n",
    "# Creating df from JSON file\n",
    "df2 = spark.read.option(\"header\",True).json(\"/src/resources/file.json\")\n",
    "\n",
    "# Creating df from parquet file\n",
    "df2=spark.read.parquet(\"/temp/out/people.parquet\")\n",
    "\n",
    "################################################## Write ####################################################################\n",
    "\n",
    "# Write DataFrame to Externals files\n",
    "\n",
    "# CSV\n",
    "df.write.csv('dataset.csv')\n",
    "\n",
    "# JSON\n",
    "data.write.save('dataset.json', format='json')\n",
    "\n",
    "# Parquet\n",
    "data.write.save('dataset.parquet', format='parquet')\n",
    "\n",
    "# write data to parquest\n",
    "df.write.parquet(\"/tmp/output/people.parquet\")\n",
    "\n",
    "# Append or Overwrite an existing Parquet file\n",
    "df.write.mode('append').parquet(\"/tmp/output/people.parquet\")\n",
    "df.write.mode('overwrite').parquet(\"/tmp/output/people.parquet\")\n",
    "\n",
    "# Create Parquet partition file\n",
    "df.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(\"/tmp/output/people2.parquet\")\n",
    "\n",
    "## Writing selected data to different file formats\n",
    "\n",
    "# CSV\n",
    "data.select(['data', 'open', 'close', 'adjusted']).write.csv('dataset.csv')\n",
    "\n",
    "# JSON\n",
    "data.select(['data', 'open', 'close', 'adjusted']).write.save('dataset.json', format='json')\n",
    "\n",
    "# Parquet\n",
    "data.select(['data', 'open', 'close', 'adjusted']).write.save('dataset.parquet', format='parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame\n",
    "df=spark.createDataFrame(data,columns)\n",
    "\n",
    "# creates a DataFrame with one column id\n",
    "df = spark.range(10) \n",
    "\n",
    "# creates a DataFrame\n",
    "df = spark.sql(\"show tables\")\n",
    "\n",
    "# printSchema\n",
    "df.printSchema()\n",
    "\n",
    "# Checking if a Column Exists in a DataFrame\n",
    "df.schema.fieldNames.contains(\"firstname\")\n",
    "df.schema.contains(StructField(\"firstname\",StringType,true))\n",
    "\n",
    "# CHeck the data types of the column\n",
    "df.select(\"petal_width\").dtypes\n",
    "\n",
    "# chnage the datatype of column in Dataframe\n",
    "df = df.withColumn(\"Quantity\",df.Quantity.cast('int'))\n",
    "df = df.withColumn(\"Calories\",df['calories'].cast(\"Integer\"))\n",
    "\n",
    "# Convert Spark Datafrme to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Display 2 rows and full column contents\n",
    "df.show(2,truncate=False) \n",
    "\n",
    "# Display 2 rows & column values 25 characters\n",
    "df.show(2,truncate=25) \n",
    "\n",
    "# Display DataFrame rows & columns vertically\n",
    "df.show(n=3,truncate=25,vertical=True)\n",
    "\n",
    "# Get the rows in the dataframe as list of row\n",
    "df.take(2)\n",
    "\n",
    "# Change the value of Existing Column\n",
    "from pyspark.sql.functions import col\n",
    "df_value = df.withColumn(\"petal_length\",col(\"petal_length\") * 10)\n",
    "\n",
    "# Rename the Column of DataFrame\n",
    "re_df = df.withColumnRenamed(\"petal_width\",\"petal_width_1\")\n",
    "\n",
    "# Add new column with constant value using 'lit'\n",
    "from pyspark.sql.functions import lit\n",
    "new_col = df.withColumn(\"COllege\",lit(\"MITRC\"))\n",
    "\n",
    "#Drop column\n",
    "drop_df = df.drop(\"COllege\")\n",
    "\n",
    "# greatest\n",
    "from pyspark.sql.functions import greatest,col\n",
    "df1=df.withColumn(\"large\",greatest(col(\"level1\"),col(\"level2\"),col(\"level3\"),col(\"level4\")))\n",
    "\n",
    "# least\n",
    "from pyspark.sql.functions import least,col\n",
    "df2=df.withColumn(\"Small\",least(col(\"level1\"),col(\"level2\"),col(\"level3\"),col(\"level4\")))\n",
    "\n",
    "# Capitalize the first letter of the user name\n",
    "from pyspark.sql.functions import initcap\n",
    "df = df.withColumn('name_cap', initcap('user_name'))\n",
    "\n",
    "\n",
    "############################################### Missing and Fill values #######################################################\n",
    "\n",
    "# Drop Duplicates\n",
    "df = df.dropDuplicates(['user_id']).orderBy('age')\n",
    "\n",
    "# Replace Null/None Value with Empty String\n",
    "df.na.fill(\"\").show(false)\n",
    "\n",
    "#Replace 0 for null for all integer columns\n",
    "df.na.fill(value=0).show()\n",
    "\n",
    "#Replace Replace 0 for null on only population column \n",
    "df.na.fill(value=0,subset=[\"population\"]).show()\n",
    "\n",
    "df.na.fill(\"unknown\",[\"city\"]).na.fill(\"\",[\"type\"]).show()\n",
    "\n",
    "df.na.fill({\"city\": \"unknown\", \"type\": \"\"}).show()\n",
    "\n",
    "##################################################### Select ##################################################################\n",
    "# Show specific column data\n",
    "df.select(\"firstname\").show()\n",
    "df.select(\"firstname\",\"lastname\").show()\n",
    "\n",
    "# Select All columns from List\n",
    "columns = ['firstname','lastname','country','state']\n",
    "df.select(*columns).show()\n",
    "\n",
    "df.select([col for col in df.columns]).show()\n",
    "df.select(\"*\").show()\n",
    "\n",
    "# Select Columns by Index\n",
    "df.select(df.columns[:3]).show(3)\n",
    "\n",
    "#Selects columns 2 to 4  and top 3 rows\n",
    "df.select(df.columns[2:4]).show(3)\n",
    "\n",
    "\n",
    "##################################################### Aggregation ##############################################################\n",
    "\n",
    "# Average\n",
    "from pyspark.sql.functions import avg\n",
    "df_avg.select(avg(\"Total Cost\").alias(\"Average Cost\")).show()\n",
    "\n",
    "# sum\n",
    "from pyspark.sql.functions import sum\n",
    "df_avg.select(sum(\"Quantity\").alias(\"Total Items\")).show()\n",
    "\n",
    "# groupBy\n",
    "df.groupBy(\"Shop_Name\").sum(\"Quantity\").show()\n",
    "\n",
    "#Average money earn\n",
    "df_avg.groupBy(\"Shop_Name\").avg(\"Total Cost\").show()\n",
    "\n",
    "# max\n",
    "from pyspark.sql.functions import max\n",
    "df.select(max(\"Quantity\").alias(\"Maximum Quantity\")).show()\n",
    "\n",
    "# min\n",
    "from pyspark.sql.functions import min\n",
    "df.groupBy(\"Shop_Name\").min(\"Quantity\").show()\n",
    "\n",
    "# count\n",
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"Quantity\")).show()\n",
    "\n",
    "# distinct\n",
    "df.select(df[\"Shop_Name\"]).distinct().show()\n",
    "\n",
    "# collection list\n",
    "from pyspark.sql.functions import collect_list\n",
    "df.select(collect_list(\"Shop_Name\")).show(truncate=False)\n",
    "\n",
    "# set\n",
    "from pyspark.sql.functions import collect_set\n",
    "df.select(collect_set(\"Shop_Name\")).show(truncate=False)\n",
    "\n",
    "# countDistinct\n",
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"Shop_Name\")).show()\n",
    "\n",
    "# Kurthosis\n",
    "from pyspark.sql.functions import kurtosis\n",
    "df.select(kurtosis('Quantity')).show(truncate=False)\n",
    "\n",
    "# mean\n",
    "from pyspark.sql.functions import mean\n",
    "df.select(mean('Quantity')).show(truncate=False)\n",
    "\n",
    "# skewness function\n",
    "from pyspark.sql.functions import skewness\n",
    "df.select(skewness('Quantity')).show(truncate=False)\n",
    "\n",
    "# stddev\n",
    "from pyspark.sql.functions import stddev,stddev_samp,stddev_pop\n",
    "df.select(stddev(\"Quantity\"), stddev_samp(\"Quantity\"),stddev_pop(\"Quantity\")).show(truncate=False)\n",
    "\n",
    "# sumDistinct\n",
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show(truncate=False)\n",
    "\n",
    "# Variance\n",
    "from pyspark.sql.functions import variance,var_samp,var_pop\n",
    "df.select(variance(\"Quantity\"),var_samp(\"Quantity\"),var_pop(\"Quantity\")).show(truncate=False)\n",
    "\n",
    "\n",
    "##################################################### Filters ################################################################\n",
    "\n",
    "# Like SQL Expression\n",
    "from pyspark.sql.functions import filter\n",
    "from pyspark.sql.functions import col\n",
    "df.filter(col(\"Dept No\") ==1).show()\n",
    "\n",
    "\n",
    "# Filtering with multiple condition AND(&&);NOT(!) OR(||)\n",
    "from pyspark.sql.functions import filter\n",
    "df.filter((df[\"Gender\"]==\"M\") & (df[\"Dept No\"]==2)).show()\n",
    "\n",
    "# Filtering on an array column\n",
    "from pyspark.sql.functions import filter\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.filter(array_contains(df[\"Languages\"],\"Python\")).show()\n",
    "\n",
    "\n",
    "#filter data by null values\n",
    "from pyspark.sql.functions import filter\n",
    "from pyspark.sql.functions import isNotNull\n",
    "df.filter(df.name.isNotNull()).show()\n",
    "\n",
    "from pyspark.sql.functions import filter\n",
    "from pyspark.sql.functions import isNull\n",
    "df.filter(df.name.isNull()).show()\n",
    "\n",
    "##################################################### When ################################################################\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "df.select(\"name\", when(df.vitamins >= \"25\", \"rich in vitamins\")).show()\n",
    "\n",
    "from pyspark.sql.functions import col,when\n",
    "df_when  = df.withColumn('Gender',when(col(\"Gender\") == \"M\",\"Male\").when(col(\"Gender\") == \"F\",\"Female\").otherwise(\"Other\"))\n",
    "df_when.show()\n",
    "\n",
    "df_when2  = df.select(col(\"*\"),when(col(\"Gender\") == \"M\",\"Male\").when(col(\"Gender\") == \"F\",\"Female\").otherwise(\"Unknow\")\n",
    "                                                                                                    .alias(\"New_gender\"))\n",
    "\n",
    "################################################ Sort and OrderBy #############################################################\n",
    "# Sort the dataframe\n",
    "df.sort(\"department\",\"state\").show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df.sort(col(\"department\"),col(\"state\")).show(truncate=False)\n",
    "\n",
    "df.orderBy(\"department\",\"state\").show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df.orderBy(col(\"department\"),col(\"state\")).show(truncate=False)\n",
    "\n",
    "# Sort by Ascending (ASC)\n",
    "df.sort(df.department.asc(),df.state.asc()).show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df.sort(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n",
    "\n",
    "# Sort by Descending (DESC)\n",
    "df.sort(df.department.asc(),df.state.desc()).show(truncate=False)\n",
    "\n",
    "df.sort(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\n",
    "\n",
    "df.orderBy(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\n",
    "\n",
    "################################################ Window function #############################################################\n",
    "\n",
    "# row_number \n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)).show(truncate=False)\n",
    "\n",
    "# rank\n",
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)).show()\n",
    "\n",
    "# dense_rank\n",
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)).show()\n",
    "\n",
    "# percent_rank\n",
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)).show()\n",
    "\n",
    "# ntile\n",
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)).show()\n",
    "\n",
    "# cume_dist\n",
    "from pyspark.sql.functions import cume_dist    \n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)).show()\n",
    "\n",
    "# lag\n",
    "from pyspark.sql.functions import lag    \n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)).show()\n",
    "\n",
    "# lead\n",
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)).show()\n",
    "\n",
    "\n",
    "####################################################### Joins ################################################################\n",
    "\n",
    "# Employee dataframe\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "\n",
    "# Department dataFrame\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "\n",
    "# Inner joins\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\").show(truncate=False)\n",
    "\n",
    "# Full Outer Join\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\").show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\").show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\").show(truncate=False)\n",
    "\n",
    "# Left Outer Join\n",
    "empDF.join(deptDF,empDF(\"emp_dept_id\") ==  deptDF(\"dept_id\"),\"left\").show(false)\n",
    "empDF.join(deptDF,empDF(\"emp_dept_id\") ==  deptDF(\"dept_id\"),\"leftouter\").show(false)\n",
    "\n",
    "# Right Outer Join\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\").show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\").show(truncate=False)\n",
    "\n",
    "# Left Semi Join\n",
    "\n",
    "# Left Anti Join\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\").show(truncate=False)\n",
    "\n",
    "####################################################### Union ################################################################\n",
    "\n",
    "df1 = spark.createDataFrame(data = simpleData1, schema = columns1)\n",
    "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
    "\n",
    "unionDF = df1.union(df2)\n",
    "unionDF.show(truncate=False)\n",
    "\n",
    "# Merge without Duplicates\n",
    "disDF = df.union(df2).distinct()\n",
    "disDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Expresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing SQL Expression\n",
    "df.createOrReplaceTempView(\"parquetTable\")\n",
    "parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")\n",
    "\n",
    "# using parquest path\n",
    "spark.sql(\"CREATE TEMPORARY VIEW PERSON USING parquet OPTIONS (path \\\"/tmp/output/people.parquet\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON\").show()\n",
    "\n",
    "#  Change Column type using SQL Expression\n",
    "df.createOrReplaceTempView(\"Table\")\n",
    "df_sql = spark.sql(\"SELECT STRING(species),Float(sepal_length) from Table\")\n",
    "df_sql.printSchema()\n",
    "\n",
    "df_sql.show(5)\n",
    "\n",
    "# Change Column type using selectExpr\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df_new = df.selectExpr(\"cast(petal_width as Double ) petal_width\", \"cast(sepal_length as Double) sepal_length\")\n",
    "df_new.printSchema()\n",
    "df_new.show(5)\n",
    "\n",
    "# case when\n",
    "\n",
    "# Question: Create a column “Performance” and find it out on the basis of percentage?\n",
    "from pyspark.sql.functions import expr\n",
    "df_case = df.withColumn(\"Performance\", expr(\"case when Percentage>88.0 then 'Excellent' \" \\ \n",
    "                                            + \"when Percentage<83.0 then 'Average' \"  \\\n",
    "                                            + \"else 'Great' end\"))\n",
    "# Order by \n",
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\").show(truncate=False)\n",
    "\n",
    "# Joins\n",
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")\n",
    "\n",
    "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\").show(truncate=False)\n",
    "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
