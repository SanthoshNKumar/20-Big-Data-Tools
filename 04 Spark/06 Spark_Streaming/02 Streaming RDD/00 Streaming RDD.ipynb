{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Streaming RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Stream RDD we can use 'queueStream' from 'StreamingContext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "org.apache.spark.api.python.PythonUtils.getPythonAuthSocketTimeout does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-07343afe55a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[1]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SparkByExamples.com'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmi_cims\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmi_cims\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmi_cims\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m--> 147\u001b[1;33m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmi_cims\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encryption_enabled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misEncryptionEnabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SPARK_AUTH_SOCKET_TIMEOUT\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetPythonAuthSocketTimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SPARK_BUFFER_SIZE\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetSparkBufferSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bmi_cims\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1534\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1535\u001b[0m             raise Py4JError(\n\u001b[1;32m-> 1536\u001b[1;33m                 \"{0}.{1} does not exist in the JVM\".format(self._fqn, name))\n\u001b[0m\u001b[0;32m   1537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1538\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JError\u001b[0m: org.apache.spark.api.python.PythonUtils.getPythonAuthSocketTimeout does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "#1. Create Spark Session\n",
    "\n",
    "# Testing pyspark Intallation\n",
    "import findspark\n",
    "findspark.init('C:\\Spark')\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are trying to find the length of the names in the different rdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the chars in the name: [8]\n",
      "Length of the chars in the name: [5]\n",
      "Length of the chars in the name: [9]\n"
     ]
    }
   ],
   "source": [
    "# here we are trying to understsnd how rdds works manually\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(['santhosh'])\n",
    "rdd2 = spark.sparkContext.parallelize(['anand'])\n",
    "rdd3 = spark.sparkContext.parallelize(['Prashanth'])\n",
    "\n",
    "rdds = [rdd1,rdd2,rdd3]\n",
    "\n",
    "for rdd in rdds:\n",
    "    print(\"Length of the chars in the name:\",rdd.map(lambda x : len(x)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bmi_cims\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\bmi_cims\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "c:\\users\\bmi_cims\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c7194aa30e04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mssc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# batch duration = 0.5 seconds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0minputStream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mssc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueueStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Streaming the rdds  using 'queueStream'\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "ssc = StreamingContext(spark.sparkContext, 0.5)  # batch duration = 0.5 seconds\n",
    "\n",
    "inputStream = ssc.queueStream(rdds)\n",
    "\n",
    "mappedStream = inputStream.map(lambda x : len(x))\n",
    "mappedStream.pprint()\n",
    "\n",
    "# Here streaming will run for 6 seconds as there is sleep of 6 seconds before streaming stops\n",
    "ssc.start()\n",
    "time.sleep(6)\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Win10.eu2d4xqpkiwuzc50gvspb3qxbb.bx.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkByExamples.com</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ad6b497470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Create Spark Session\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rddQueue:[0 to 1]\n",
      "rddQueue:[1 to 2]\n",
      "rddQueue:[2 to 3]\n",
      "rddQueue:[3 to 4]\n",
      "rddQueue:[4 to 5]\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:25:47\n",
      "-------------------------------------------\n",
      "Input: 0\n",
      "Input: 1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:25:47\n",
      "-------------------------------------------\n",
      "Output: 1\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:25:48\n",
      "-------------------------------------------\n",
      "Input: 1\n",
      "Input: 2\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:25:48\n",
      "-------------------------------------------\n",
      "Output: 3\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:25:49\n",
      "-------------------------------------------\n",
      "Input: 2\n",
      "Input: 3\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:25:49\n",
      "-------------------------------------------\n",
      "Output: 5\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:25:50\n",
      "-------------------------------------------\n",
      "Input: 3\n",
      "Input: 4\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:25:50\n",
      "-------------------------------------------\n",
      "Output: 7\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:25:51\n",
      "-------------------------------------------\n",
      "Input: 4\n",
      "Input: 5\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:25:51\n",
      "-------------------------------------------\n",
      "Output: 9\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:25:52\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:25:52\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:25:53\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:25:53\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from operator import add, sub\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from time import sleep\n",
    "\n",
    "# Set up the Spark context and the streaming context\n",
    "ssc = StreamingContext(spark.sparkContext, 1)\n",
    "\n",
    "# Input data\n",
    "rddQueue = []\n",
    "for i in range(5):\n",
    "    print(\"rddQueue:[{0} to {1}]\".format(i,i+1))\n",
    "    rddQueue += [ssc.sparkContext.parallelize([i, i+1])]\n",
    "    \n",
    "inputStream = ssc.queueStream(rddQueue)\n",
    "\n",
    "inputStream.map(lambda x: \"Input: \" + str(x)).pprint()\n",
    "inputStream.reduce(add).map(lambda x: \"Output: \" + str(x)).pprint()\n",
    "\n",
    "ssc.start()\n",
    "sleep(5)\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Win10.eu2d4xqpkiwuzc50gvspb3qxbb.bx.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkByExamples.com</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ad6b486ef0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Create Spark Session\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TCP Sockets. A connection between two computers uses a socket. A socket is the combination of IP address plus port.\n",
    "\n",
    "'''\n",
    "1. Spark context which we have wrapped inside a streaming context\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:27:18\n",
      "-------------------------------------------\n",
      "6\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:27:19\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:27:20\n",
      "-------------------------------------------\n",
      "12\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:27:21\n",
      "-------------------------------------------\n",
      "25\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:27:22\n",
      "-------------------------------------------\n",
      "10\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:27:23\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:27:24\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from operator import add, sub\n",
    "from time import sleep\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Set up the Spark context and the streaming context\n",
    "ssc = StreamingContext(spark.sparkContext, 1)\n",
    "\n",
    "inputData = [\n",
    "    [1,2,3],\n",
    "    [0],\n",
    "    [4,4,4],\n",
    "    [0,0,0,25],\n",
    "    [1,-1,10],\n",
    "]\n",
    "\n",
    "rddQueue = []\n",
    "for datum in inputData:\n",
    "    rddQueue += [ssc.sparkContext.parallelize(datum)]\n",
    "    \n",
    "inputStream = ssc.queueStream(rddQueue)\n",
    "inputStream.reduce(add).pprint()\n",
    "\n",
    "ssc.start()\n",
    "sleep(5)\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Win10.eu2d4xqpkiwuzc50gvspb3qxbb.bx.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkByExamples.com</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ad6b5a3320>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from time import sleep\n",
    "\n",
    "ssc = StreamingContext(spark.sparkContext, 3) # batchDuration = 3\n",
    "    \n",
    "rddQueue = []\n",
    "for i in range(3):\n",
    "    print([j for j in range(1, 21)])\n",
    "    rddQueue += [ssc.sparkContext.parallelize([j for j in range(1, 21)],10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:29:48\n",
      "-------------------------------------------\n",
      "(1, 2)\n",
      "(2, 2)\n",
      "(3, 2)\n",
      "(4, 2)\n",
      "(5, 2)\n",
      "(6, 2)\n",
      "(7, 2)\n",
      "(8, 2)\n",
      "(9, 2)\n",
      "(0, 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-16 15:29:51\n",
      "-------------------------------------------\n",
      "(1, 2)\n",
      "(2, 2)\n",
      "(3, 2)\n",
      "(4, 2)\n",
      "(5, 2)\n",
      "(6, 2)\n",
      "(7, 2)\n",
      "(8, 2)\n",
      "(9, 2)\n",
      "(0, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputStream = ssc.queueStream(rddQueue)\n",
    "mappedStream = inputStream.map(lambda x: (x % 10, 1))\n",
    "\n",
    "reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n",
    "reducedStream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "\n",
    "#sleep(6)\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Win10.eu2d4xqpkiwuzc50gvspb3qxbb.bx.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkByExamples.com</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2089cab9128>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from time import sleep\n",
    "\n",
    "ssc = StreamingContext(spark.sparkContext, 3) # batchDuration = 3\n",
    "\n",
    "rdd1 = ssc.sparkContext.parallelize([1,2,3])\n",
    "rdd2 = ssc.sparkContext.parallelize([4,5,6])\n",
    "rddQueue = [rdd1,rdd2]\n",
    "\n",
    "# Creates a DStream from the RDDs above\n",
    "numsDStream = ssc.queueStream(rddQueue)\n",
    "plusOneDStream = numsDStream.map(lambda x : x+1)\n",
    "plusOneDStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonRdd = ssc.sparkContext.parallelize([7,8,9])\n",
    "# TODO: Use the transform function to apply the union function to the RDDs within numsDStream and elements of commonRdd\n",
    "# and print the resulting DStream\n",
    "combinedDStream = numsDStream.transform(lambda rdd: rdd.union(commonRdd))\n",
    "combinedDStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:12\n",
      "-------------------------------------------\n",
      "2\n",
      "3\n",
      "4\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:12\n",
      "-------------------------------------------\n",
      "1\n",
      "2\n",
      "3\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:15\n",
      "-------------------------------------------\n",
      "5\n",
      "6\n",
      "7\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:15\n",
      "-------------------------------------------\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:18\n",
      "-------------------------------------------\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:21\n",
      "-------------------------------------------\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:24\n",
      "-------------------------------------------\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:27\n",
      "-------------------------------------------\n",
      "7\n",
      "8\n",
      "9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start() \n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:30\n",
      "-------------------------------------------\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:33\n",
      "-------------------------------------------\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:36\n",
      "-------------------------------------------\n",
      "7\n",
      "8\n",
      "9\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-01-17 03:49:39\n",
      "-------------------------------------------\n",
      "7\n",
      "8\n",
      "9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
