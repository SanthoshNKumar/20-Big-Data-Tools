{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spark Streaming has 3 major components: \n",
    "    - Input sources\n",
    "    - Streaming engine \n",
    "    - Sink. \n",
    "\n",
    "    1. Input sources generate data like Kafka, Flume, HDFS/S3, etc.\n",
    "    \n",
    "    2. Spark Streaming engine processes incoming data from various input sources.\n",
    "    \n",
    "    3. Sinks store processed data from Spark Streaming engine like HDFS, relational databases, or NoSQL datastores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spark will process data in micro-batches which can be defined by triggers.\n",
    "\n",
    "Example : Let's say we define a trigger as 1 second, this means Spark will create micro-batches every second and process \n",
    "them accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Output Mode:\n",
    "    - After processing the streaming data, Spark needs to store it somewhere on persistent storage. \n",
    "    - Spark uses various output modes to store the streaming data.\n",
    "        1. Append Mode\n",
    "        2. Update Mode\n",
    "        3. Complete Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Window functions:\n",
    "    \n",
    "    Aggregate: min, max, avg, count, and sum.\n",
    "    \n",
    "    Ranking: rank, dense_rank, percent_rank, row_num, and ntile\n",
    "    \n",
    "    Analytical: cume_dist, lag, and lead\n",
    "    \n",
    "    Custom boundary: rangeBetween and rowsBetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input Sources\n",
    "\n",
    "    Spark Streaming ingests data from different types of input sources for processing in real-time.\n",
    "    \n",
    "Rate (for Testing): \n",
    "    - It will automatically generate data including 2 columns timestamp and value . \n",
    "    - This is generally used for testing purposes. We demonstrated this in part 1 of this series.\n",
    "\n",
    "Socket (for Testing): \n",
    "    - This data source will listen to the specified socket and ingest any data into Spark Streaming. \n",
    "          It is also used only for testing purposes.\n",
    "\n",
    "File: \n",
    "    - This will listen to a particular directory as streaming data. \n",
    "    - It supports file formats like CSV, JSON, ORC, and Parquet. You can find the latest supported file format list here.\n",
    "\n",
    "Kafka: \n",
    "    - This will read data from Apache Kafka® and is compatible with Kafka broker versions 0.10.0 or higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Output Sinks\n",
    "    In Spark Streaming, output sinks store results into external storage.\n",
    "    \n",
    "    Console sink: \n",
    "        - Displays the content of the DataFrame to console. In this series, we have only used console sink, refer to previous \n",
    "          posts for details.\n",
    "\n",
    "    File sink: \n",
    "        - Stores the contents of a DataFrame in a file within a directory. Supported file formats are csv, json, orc, and \n",
    "          parquet.\n",
    "\n",
    "    Kafka sink: \n",
    "        - Publishes data to a Kafka® topic.\n",
    "\n",
    "    Foreach sink: \n",
    "        - Applies to each row of a DataFrame and can be used when writing custom logic to store data.\n",
    "\n",
    "    ForeachBatch sink: \n",
    "        - Applies to each micro-batch of a DataFrame and also can be used when writing custom logic to store data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "01 Streaming RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "02 Streaming Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "03 Streaming Window Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "04 Streaming Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "05 UpdateStateByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "06 Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "07 Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "07 Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "08 Kafka nand AWS Kinesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
