{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Set and Install Spark on hadoop\n",
    "    1.1 Install Java 8 runtime environment (JRE)\n",
    "    1.2 Java 8 development Kit (JDK)\n",
    "    1.3 Download map hadoop Binaries\n",
    "    1.4 Setting up environment variables for 'Java' and 'hadoop'\n",
    "    1.5 Configuring Hadoop cluster\n",
    "    1.6 Starting Hadoop services (Yarn service/Master/Slave)\n",
    "    1.7 Check the Hadoop Web UI\n",
    "    1.9 Install Spark\n",
    "    1.10 Download map hadoop Binaries\n",
    "    1.11 Start Spark Services(Master\\slaves)\n",
    "    1.12 Web GUI for Spark\n",
    "    1.13 Run sample Python code using Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. Understading Spark\n",
    "    2.1 How spark was Invented?\n",
    "    2.2 Features of Spark\n",
    "    2.3 Cluster Management in Spark\n",
    "    2.4 Spark architecture\n",
    "            - Driver\n",
    "            - Exector\n",
    "            - SparkContext\n",
    "            - Application Master\n",
    "            - Cluster Resource Manager            \n",
    "    2.5 Yarn - Cluster Mode\n",
    "    \n",
    "    2.8 SparkConf\n",
    "        1. setMaster\n",
    "        2. setAppName\n",
    "        3. setExecutorEnv\n",
    "        4. get\n",
    "        5. getAll\n",
    "        6. contains\n",
    "        7. setSparkHome\n",
    "\n",
    "    2.9 SparkSession\n",
    "        1. Entry Points\n",
    "            1. SparkContext\n",
    "            2. SQLContext\n",
    "            3. Streaming Context\n",
    "            4. Hive Context\n",
    "        \n",
    "        2. createDataFrame\n",
    "            \n",
    "    2.8 SparkContext\n",
    "        1. parallelize\n",
    "        2. Create Empty rdd\n",
    "\n",
    "    2.6 RDD\n",
    "         1. RDD Operation\n",
    "             1. Transformation\n",
    "             2. Actions\n",
    "         2. coalesce and partition\n",
    "         3. cache and persist\n",
    "         \n",
    "    2.7 DAG\n",
    "            \n",
    "    2.10 Difference between' MapReduce' and 'Spark'\n",
    "    \n",
    "    2.11 Limitation of using Spark in applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3. Write pyspark Application two modes\n",
    "    1. Interactive  -- Pysaprk Shell\n",
    "    2. Batch Application -- (Jupyter notebook, Pycharm, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4. SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "6. SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "7. Understanding PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "8. RDD\n",
    "    8.1 Understanding RDD\n",
    "    8.2 Operations in RDD\n",
    "        - Transformation\n",
    "            - map\n",
    "            - filter\n",
    "            - flatmap\n",
    "            - mapPartitions\n",
    "            - mapPartitionsWithIndex\n",
    "            - sample\n",
    "            - union\n",
    "            - intersection\n",
    "            - distinct\n",
    "            - groupByKey\n",
    "            - fold\n",
    "            - aggregate\n",
    "            - reduce\n",
    "            - reduceByKey\n",
    "            - sortBykey\n",
    "            - join\n",
    "            - cartesian\n",
    "            - coalesce\n",
    "            - repartition\n",
    "            - repartitionAndSortWithinPartitions\n",
    "        - Actions\n",
    "            - reduce\n",
    "            - colleect\n",
    "            - aggregate\n",
    "            - count\n",
    "            - first\n",
    "            - take\n",
    "            - takeSample \n",
    "            - takeOrdered\n",
    "            - countApprox\n",
    "            - countApproxDistict\n",
    "            - countByValue\n",
    "            - countByValueApprox\n",
    "            - countByKey\n",
    "            - fold\n",
    "            - foreach\n",
    "            - foreachpartition\n",
    "            - min\n",
    "            - max\n",
    "            - treeAggregator\n",
    "            - treeReduce\n",
    "            - saveAsTextFile\n",
    "            - saveAsObjectFile\n",
    "            - saveAsSequenceFile\n",
    "    8.3 parallelize\n",
    "    8.4 Repartition() vs Coalesce()\n",
    "    8.5 Shuffle Partitions\n",
    "    8.6 cache and Persist\n",
    "    8.7 Persistance Storage Levels\n",
    "    8.8 Broadcast and Accumulator variables\n",
    "    8.9 Convert RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "9. DataSource\n",
    "    9.1 Read and Write CSV file\n",
    "    9.2 Read and write Json file\n",
    "    9.3 Read and write Parquest file\n",
    "    9.4 Read and write XML file\n",
    "    9.5 Read and write Avro file\n",
    "    9.6 Read and write HBase\n",
    "    9.7 Read and write OCR File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "10. DataFrame\n",
    "    10.1 Create DataFrame\n",
    "    10.2 Create Empty DataFrame\n",
    "    10.3 Convert RDD to DataFrame\n",
    "    10.4 Convert DataFrame to Pandas\n",
    "    10.5 Inspect Data\n",
    "            1. dtypes\n",
    "            2. show\n",
    "            3. head\n",
    "            4. first\n",
    "            5. take\n",
    "            6. describe\n",
    "            7. columns\n",
    "            8. count\n",
    "            9. explain\n",
    "    10.6 StructType & StructField\n",
    "    10.7 Row and Column class\n",
    "    10.8 Missing & Replacing Values\n",
    "            1. fill and fill na \n",
    "            3. Drop and drop Duplicates\n",
    "            4. dropna\n",
    "            5. replace\n",
    "    10.9 Querying\n",
    "            1. “Select” Operation\n",
    "            2. “When” Operation\n",
    "            3. \"isin\"\n",
    "            3. “Like” Operation\n",
    "            4. “Startswith” — “ Endswith”\n",
    "            5. “Substring” Operation\n",
    "    10.10 Filter\n",
    "            1. filter() with Column Condition\n",
    "            2. filter() with SQL Expression\n",
    "            3. Filter with Multiple Conditions\n",
    "            4. Based on List Values\n",
    "            5. Filter like and rlike\n",
    "            7. Array column\n",
    "            8. Filtering on Nested Struct columns\n",
    "            9. Source code of PySpark where filter\n",
    "    10.1 Groupby\n",
    "        1. count\n",
    "        2. sum\n",
    "        3. min\n",
    "        4. max\n",
    "        5. avg\n",
    "        6. mean\n",
    "    10.11 join\n",
    "    10.12 union and unionAll\n",
    "    10.13 OrderBy and Sort\n",
    "    10.14 map\n",
    "    10.15 foreach\n",
    "    10.16 sample and sampleby\n",
    "    10.18 partitionBy\n",
    "    10.19 MapType\n",
    "    10.20 Repartitioning\n",
    "            1. repartition\n",
    "            2. coalesce\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "11. SQL Functions \n",
    "        ** These are the functions can apply on DataFrme (dont confuse with word SQL mean not sql query)\n",
    "\n",
    "    - Aggregation Function\n",
    "        11.1 approx_count_distinct\n",
    "        11.2 avg\n",
    "        11.3 collect_list\n",
    "        11.4 collect_set\n",
    "        11.5 countDistinct\n",
    "        11.6 count\n",
    "        11.7 grouping\n",
    "        11.8 first\n",
    "        11.9 last\n",
    "        11.10 kurtosis\n",
    "        11.11 max\n",
    "        11.12 min\n",
    "        11.13 mean\n",
    "        11.14 skewness\n",
    "        11.15 stddev\n",
    "        11.16 stddev_samp\n",
    "        11.17 stddev_pop\n",
    "        11.18 sum\n",
    "        11.19 sumDistinct\n",
    "        11.20variance, var_samp, var_pop\n",
    "    \n",
    "    - Window Functions\n",
    "        11.1 ranking functions\n",
    "        11.2 analytic functions\n",
    "        11.3 aggregate functions\n",
    "        \n",
    "    - Built in Functions\n",
    "        11.1 When\n",
    "        11.2 expr\n",
    "        11.3 lit\n",
    "        11.4 split\n",
    "        11.5 concat_ws\n",
    "        11.6 substring\n",
    "        11.7 translate\n",
    "        11.8 regexp_replace\n",
    "        11.9 overlay\n",
    "        11.10 to_timestamp\n",
    "        11.11 to_date\n",
    "        11.12 date_format\n",
    "        11.13 datdiff\n",
    "        11.14 months_between\n",
    "        11.15 explode\n",
    "        11.16 array_contains\n",
    "        11.17 array\n",
    "        11.18 collect_list\n",
    "        11.19 collect_set\n",
    "        11.20 create_map\n",
    "        11.21 map_key\n",
    "        11.22 map_values\n",
    "        11.23 struct\n",
    "        11.24 countDistinct\n",
    "        11.25 sum,avg,row_number,rank,dense_rank,percent_rank,typesLit,from_json,to_json,json_tuple\n",
    "        11.26 get_json_object,schema_of_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "12 Pandas API for Spark\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
