{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PySpark RDD\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark RDD to DataFrame\n",
    "\n",
    "    #1. Using rdd.toDF() function\n",
    "        df = rdd.toDF()\n",
    "        df.printSchema()\n",
    "        df.show(truncate=False)\n",
    "        \n",
    "        # Note: By default, toDF() function creates column names as “_1” and “_2”\n",
    "        \n",
    "    #2. toDF() has another signature that takes arguments to define column names as\n",
    "        deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "        df2 = rdd.toDF(deptColumns)\n",
    "        df2.printSchema()\n",
    "        df2.show(truncate=False)\n",
    "        \n",
    "    #3. Using PySpark createDataFrame() function\n",
    "        deptDF = spark.createDataFrame(rdd, schema = deptColumns)\n",
    "        deptDF.printSchema()\n",
    "        deptDF.show(truncate=False)\n",
    "        \n",
    "    #4. Using createDataFrame() with StructType schema\n",
    "        from pyspark.sql.types import StructType,StructField, StringType\n",
    "        deptSchema = StructType([StructField('dept_name', StringType(), True),StructField('dept_id', StringType(), True)])\n",
    "        deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)\n",
    "        deptDF1.printSchema()\n",
    "        deptDF1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Example\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "\n",
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "df2 = rdd.toDF(deptColumns)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "deptDF = spark.createDataFrame(rdd, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "deptSchema = StructType([       \n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('dept_id', StringType(), True)\n",
    "])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates Empty RDD\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "print(emptyRDD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
