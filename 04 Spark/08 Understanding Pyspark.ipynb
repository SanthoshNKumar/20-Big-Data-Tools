{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PySpark:\n",
    "    - PySpark is a Python API for Apache Spark.\n",
    "    - Apache Spark is an analytical processing engine for large scale powerful distributed data processing and machine learning \n",
    "      applications.\n",
    "      \n",
    "    - PySpark is a Spark library written in Python to run Python application using Apache Spark capabilities, using \n",
    "      PySpark we can run applications parallelly on the distributed cluster (multiple nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PySpark Features\n",
    "    - In-memory Computation\n",
    "    \n",
    "    - Distributed processing using parallelize\n",
    "        - RDD and DataFrame both are distributed in nature.\n",
    "    \n",
    "    - Can be used with many cluster managers (Spark, Yarn, Mesos e.t.c)\n",
    "    \n",
    "    - Fault-tolerant\n",
    "    \n",
    "    - Immutable\n",
    "        - We can create DataFrame / RDD once but can’t change it. And we can transform a DataFrame / RDD  after \n",
    "          applying transformations.\n",
    "    \n",
    "    - Lazy evaluation\n",
    "        - Which means that a task is not executed until an action is performed.\n",
    "        \n",
    "    - Cache & persistence\n",
    "    - Inbuild-optimization when using DataFrames\n",
    "    - Supports ANSI SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Why Spark?\n",
    "\n",
    "    - In the industry, there is a need for a general-purpose cluster computing tool as:\n",
    "        1. Hadoop MapReduce can only perform batch processing.\n",
    "        2. Apache Storm / S4 can only perform stream processing.\n",
    "        3. Apache Impala / Apache Tez can only perform interactive processing\n",
    "        4. Neo4j / Apache Giraph can only perform graph processing\n",
    "        \n",
    "    - Hence in the industry, there is a big demand for a powerful engine that can process the data in real-time (streaming) \n",
    "      as well as in batch mode.\n",
    "      \n",
    "Apache Spark Definition says it is a powerful open-source engine that provides real-time stream processing, interactive \n",
    "processing, graph processing, in-memory processing as well as batch processing with very fast speed, ease of use and \n",
    "standard interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RDD: Resilient Distributed Dataset \n",
    "    - RDD is the basic building block of Spark\n",
    "    - RDD is the fundamental data structure of Spark\n",
    "    - It is an immutable,partitioned collection of elements that can be operated on in parallel\n",
    "    \n",
    "More number of partitioned are there means more number of parallel taks we will have.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PySpark Modules & Packages\n",
    "    - PySpark RDD \n",
    "    - PySpark DataFrame and SQL\n",
    "    - PySpark Streaming\n",
    "    - PySpark MLib\n",
    "    - PySpark GraphFrames\n",
    "    - PySpark Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PySpark Installation\n",
    "\n",
    "    PySpark = Python + Spark\n",
    "    \n",
    "1. Java installation\n",
    "    - Set JAVA_HOME Environment varible\n",
    "    - Set New PATH\n",
    "\n",
    "2. Installing Pyspark\n",
    "    - Download Spark release and package type (.tgz) file\n",
    "    - Make a new folder called 'spark' in the C directory and extract the given file by using 'Winrar'\n",
    "    - Download and setup winutils.exe\n",
    "    - Setup Environment variables hadoop_home and Spark_home\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PySpark Architecture\n",
    "    - Apache Spark works in a master-slave architecture.\n",
    "    - The master is called “Driver” and slaves are called “Workers”.\n",
    "    - When you run a Spark application, Spark Driver creates a context that is an entry point to your application, \n",
    "      and all operations (transformations and actions) are executed on worker nodes, and the resources are managed by \n",
    "      Cluster Manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Subpackages in PySpark\n",
    "    - pyspark.sql      \n",
    "        - Allows working with Structured data\n",
    "        \n",
    "    - pyspark.streaming\n",
    "        - Allows working on streaming kind of data for stream processing\n",
    "        \n",
    "    - pyspark.ml\n",
    "        - Machine learning \n",
    "        \n",
    "    - pyspark.mllib\n",
    "        - Machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Different kind of algorithms in PySpark API MLlib\n",
    "\n",
    "    - mllib.classification \n",
    "    - mllib.clustering\n",
    "    - mllib.fpm \n",
    "    - mllib.linalg \n",
    "    - mllib.recommendation\n",
    "    - spark.mllib\n",
    "    - mllib.regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Two types of Spark MLLib Syntax in PySpark:\n",
    "    1. Spark 2.0 DataFrame Syntax\n",
    "    2. Spark RDD Syntax(Outdated)\n",
    "    \n",
    "\n",
    "*** Note:\n",
    "    - spark.mllib contains the original API built on top of RDDs\n",
    "    - spark.ml provides higher level API built on top of DataFrames for constructing ML pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MLlib Algorithms\n",
    "    - Basic Statistics\n",
    "    - Regression\n",
    "    - Classification\n",
    "    - Recommendation System\n",
    "    - Clustering\n",
    "    - Dimensionality Reduction\n",
    "    - Feature Extraction\n",
    "    - Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spark Machine Learning Algorithm Statistics\n",
    "    - Summary statistics\n",
    "    - Correlations\n",
    "    - Stratified sampling\n",
    "    - Hypothesis testing\n",
    "    - Random data generation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "When do we use PySpark?\n",
    "    - PySpark is widely used in the Computer Science and Machine Learning communities\n",
    "    - since many widely used data science libraries, such as NumPy, are written in Python.\n",
    "    - TensorFlow is also commonly used due to its ability to handle large datasets quickly.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Which data is Big Data?\n",
    "    - It is not considered Big Data as data will fit on a local computer on a scale of 0–32 GB based on RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parquet File?\n",
    "- Apache Parquet file is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the \n",
    "  choice of data processing framework, data model, or programming language\n",
    "- While querying columnar storage, it skips the nonrelevant data very quickly, making faster query execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PySpark StructType & StructField: \n",
    "    - PySpark StructType & StructField classes are used to programmatically specify the schema to the DataFrame and creating \n",
    "      complex columns like nested struct, array and map columns.\n",
    "    - StructType is a collection of StructField’s that defines column name,column data type, boolean to specify if the \n",
    "      field can be nullable or not and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "expr() function takes SQL expression as a string argument, executes the expression, and returns a PySpark Column type\n",
    "\n",
    "Example : \n",
    "\n",
    "df2=df.withColumn(\"gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" + \"WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "StructType & StructField \n",
    "\n",
    "- PySpark StructType & StructField classes are used to programmatically specify the schema to the DataFrame and creating \n",
    "  complex columns like nested struct, array and map columns.\n",
    "  \n",
    "- StructType is a collection of StructField’s that defines column name, column data type, boolean to specify if the \n",
    "  field can be nullable or not and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PySpark partition:\n",
    "    - PySpark partition is a way to split a large dataset into smaller datasets based on one or more partition keys. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
