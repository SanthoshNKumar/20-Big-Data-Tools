{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Context : \n",
    "\n",
    "  - SparkContext is an 'entry point' to the PySpark functionality that is used to 'communicate' with \n the cluster and to create an RDD, accumulator, \n",
    "    and broadcast variables.\n",
    "  - We can create only one 'SparkContext' per JVM, in order to create another first you need to 'stop' \n the existing one using stop() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Ways to Create Spark Context\n",
    "\n",
    "  1. Create SparkContext using its constructor and pass parameters like master and appName at least as these are mandatory params.\n",
    "      \n",
    "      # Create SparkContext\n",
    "      from pyspark import SparkContext\n",
    "      sc = SparkContext(\"local\", \"Spark_Example_App\")\n",
    "      print(sc.appName)\n",
    "\n",
    "      # Create RDD\n",
    "      rdd3 =  sc.parallelize([1,2,3])\n",
    "      rdd3.collect()\n",
    "\n",
    "  2. Create SparkContext using by pass 'SparkConf()' ref to the  getOrCreate() of the 'SparkContext'\n",
    "     \n",
    "     from pyspark import SparkConf, SparkContext\n",
    "     conf = SparkConf()  # Spark Configuration\n",
    "     conf.setMaster(\"local\").setAppName(\"Spark Example App\")\n",
    "     sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "      # Create RDD\n",
    "      rdd3 =  sc.parallelize([1,2,3])\n",
    "      rdd3.collect()\n",
    "\n",
    "  3. Create SparkContext in PySpark 'SparkSession'\n",
    "     \n",
    "     from pyspark.sql import SparkSession\n",
    "     spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "     print(spark.appName)\n",
    "\n",
    "     # Create RDD\n",
    "     rdd3 =  spark.sparkContext.parallelize([1,2,3])\n",
    "     rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session :\n",
    "  - SparkSession vs SparkContext – Since earlier versions of Spark or Pyspark, SparkContext (JavaSparkContext for Java) is an entry point to Spark \n",
    "    programming with RDD and to connect to Spark Cluster, Since Spark 2.0 SparkSession has been introduced and became an entry point to start \n",
    "    programming with DataFrame and Dataset.\n",
    "\n",
    "  - Spark Session also includes all the APIs available in different contexts\n",
    "      - Spark Context\n",
    "      - SQL Context\n",
    "      - Streaming Context\n",
    "      - Hive Context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Configuration \n",
    "  - The SparkConf offers configuration for any Spark application. \n",
    "  - To start any Spark application on a local Cluster or a dataset, we need to set some configuration and parameters, and it can be done using \n",
    "  - SparkConf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Win10.eu2d4xqpkiwuzc50gvspb3qxbb.bx.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkByExamples.com</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1da1ed37550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Create Spark Session\n",
    "\n",
    "# Testing pyspark Intallation\n",
    "import findspark\n",
    "findspark.init('C:\\Spark')\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark configuration\n",
    "from pyspark import SparkConf\n",
    "conf = SparkConf().setAppName(\"Spark Demo\").setMaster(\"local\")\n",
    "\n",
    "# pass Spark configuration on SparkContext\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SaprkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create SparkContext\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"My First Spark Application\")\n",
    "\n",
    "# Create Spark RDD using SparkContext parallelize\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "\n",
    "#Create RDD from external Data source\n",
    "rdd2 = spark.sparkContext.textFile(\"/path/textFile.txt\")\n",
    "\n",
    "#Reads entire file into a RDD as single record.\n",
    "rdd3 = spark.sparkContext.wholeTextFiles(\"/path/textFile.txt\")\n",
    "\n",
    "# Creates empty RDD with no partition    \n",
    "rdd = spark.sparkContext.emptyRDD \n",
    "# rddString = spark.sparkContext.emptyRDD[String]\n",
    "\n",
    "#Create empty RDD with partition\n",
    "rdd2 = spark.sparkContext.parallelize([],10) #This creates 10 partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Get Active Spark session\n",
    "spark.getActiveSession()\n",
    "\n",
    "# Get Spark Version\n",
    "spark.version\n",
    "\n",
    "# Get app name\n",
    "spark.sparkContext.appName\n",
    "\n",
    "# get master \n",
    "spark.sparkContext.master\n",
    "\n",
    "# Paralleize the task using Sparkcontext\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# create Empty RDD\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Broadcast varibles\n",
    "broadcastVar = spark.sparkContext.broadcast([0, 1, 2, 3])\n",
    "broadcastVar.value\n",
    "\n",
    "# Accumulator varibles\n",
    "accum = spark.sparkContext.longAccumulator(\"SumAccumulator\")\n",
    "spark.sparkContext.parallelize([1, 2, 3]).foreach(lambda x: accum.add(x))\n",
    "\n",
    "# Create DataFrame\n",
    "df=spark.createDataFrame(data,columns)\n",
    "\n",
    "# Create Empty DataFrame\n",
    "df=spark.emptyDataFrame()\n",
    "\n",
    "# Create Dataset\n",
    "df=spark.createDataset()\n",
    "\n",
    "# Create Empty DataSet\n",
    "df=spark.emptyDataFrame()\n",
    "\n",
    "# Stop Sparkcontext\n",
    "spark.stop()\n",
    "\n",
    "# Returns a SparkContext\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert RDD to dataframe\n",
    "\n",
    "# Create RDD\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Using toDF() function\n",
    "df = rdd.toDF()\n",
    "\n",
    "# Create Empty RDD\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Create Dataframe from Empty dataFrame\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([StructField('firstname', StringType(), True),\n",
    "                     StructField('middlename', StringType(), True),\n",
    "                     StructField('lastname', StringType(), True)])\n",
    "\n",
    "# Create empty DataFrame from empty RDD\n",
    "df = spark.createDataFrame(emptyRDD,schema)\n",
    "\n",
    "# Convert Empty RDD to DataFrame\n",
    "df1 = emptyRDD.toDF(schema)\n",
    "\n",
    "# Create Empty DataFrame with Schema.\n",
    "df2 = spark.createDataFrame([], schema)\n",
    "\n",
    "# Create Empty DataFrame without Schema (no columns)\n",
    "df3 = spark.createDataFrame([], StructType([]))\n",
    "\n",
    "rdd3.getNumPartitions()\n",
    "\n",
    "rdd3_coalesce = rdd3.coalesce(1)\n",
    "rdd3_coalesce.getNumPartitions()\n",
    "\n",
    "# RDD Cache\n",
    "cachedRdd = rdd.cache()\n",
    "\n",
    "dfPersist = df.persist()\n",
    "\n",
    "# RDD Persist\n",
    "import pyspark\n",
    "dfPersist = rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "dfPersist.show(false)\n",
    "    \n",
    "# RDD Unpersist\n",
    "rddPersist2 = rddPersist.unpersist()\n",
    "\n",
    "############################################## Transformations ################################################################\n",
    "\n",
    "rdd = sc.textFile(\"PATH/blogtexts\")\n",
    "rdd.take(5)\n",
    "\n",
    "# Sample\n",
    "rdd3_sampled = rdd3.sample(False, 0.4, 42)\n",
    "\n",
    "rdd3_sampled.collect()\n",
    "\n",
    "# flapmap\n",
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "# map\n",
    "rdd3 = rdd2.map(lambda x: (x,1))\n",
    "\n",
    "rdd6 = rdd5.map(lambda x: (x[1],x[0])).sortByKey()\n",
    "\n",
    "# reduceByKey\n",
    "rdd5 = rdd4.reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "# Transformation: filter\n",
    "\n",
    "# Filter\n",
    "stopwords = ['is','am','are','the','for','a']\n",
    "rdd3 = rdd2.filter(lambda x: x not in stopwords)\n",
    "\n",
    "# take\n",
    "rdd3.take(10)\n",
    "\n",
    "\n",
    "# Set Theory / Relational Transformation\n",
    "        # Transformation: union\n",
    "        # Transformation: join\n",
    "\n",
    "        # Transformation: distinct\n",
    "rdd3_distinct = rdd3.distinct()\n",
    "len(rdd3_distinct.collect())\n",
    "\n",
    "################################################# Actions ###################################################################\n",
    "\n",
    "# collect\n",
    "data = rdd6.collect()\n",
    "for f in data:\n",
    "    print(\"Key:\"+ str(f[0]) +\", Value:\"+f[1])\n",
    "\n",
    "# Reduce\n",
    "num_rdd = sc.parallelize(range(1,1000))\n",
    "num_rdd.reduce(lambda x,y: x+y)\n",
    "\n",
    "# Mathematical / Statistical Actions\n",
    "num_rdd.max(),num_rdd.min(), num_rdd.sum(),num_rdd.variance(),num_rdd.stdev() \n",
    "\n",
    "# first\n",
    "firstRec = rdd6.first()\n",
    "print(\"First Record : \"+str(firstRec[0]) + \",\"+ firstRec[1])\n",
    "\n",
    "# max\n",
    "datMax = rdd6.max()\n",
    "\n",
    "# take\n",
    "data3 = rdd6.take(3)\n",
    "for f in data3:\n",
    "    print(\"data3 Key:\"+ str(f[0]) +\", Value:\"+f[1])\n",
    "\n",
    "# saveAsTextFile\n",
    "rdd6.saveAsTextFile(\"/tmp/wordCount\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Win10.eu2d4xqpkiwuzc50gvspb3qxbb.bx.internal.cloudapp.net:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkByExamples.com</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a04c4670f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Create Spark Session\n",
    "\n",
    "# Testing pyspark Intallation\n",
    "import findspark\n",
    "findspark.init('C:\\Spark')\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "# Example\n",
    "rdd1 = spark.sparkContext.parallelize([1,2,3])\n",
    "rdd2 = spark.sparkContext.parallelize([4,5,6])\n",
    "rdd3 = spark.sparkContext.parallelize([7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[343, 512, 729]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.map(lambda x: x*x*x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 125, 216]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.map(lambda x: x*x*x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8, 9]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 27]\n",
      "[64, 125, 216]\n",
      "[343, 512, 729]\n"
     ]
    }
   ],
   "source": [
    "rdds = [rdd1,rdd2,rdd3]\n",
    "\n",
    "for rdd in rdds:\n",
    "    print(rdd.map(lambda x: x*x*x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.count()  # [1,2,3] there 3 item in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.count() # [4,5,6] there 3 item in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.reduce(lambda x, y : x + y) #sum elements in the list [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 12, 13]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.map(lambda x:x+10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.filter(lambda x: x%2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rahul', 'Rohan']\n"
     ]
    }
   ],
   "source": [
    "rdd5 = spark.sparkContext.parallelize(['Rahul', 'Swati', 'Rohan', 'Shreya', 'Priya'])\n",
    "print(rdd5.filter(lambda x: x.startswith('R')).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "rdd6 = spark.sparkContext.parallelize([2,4,5,6,7,8,9])\n",
    "union_rdd_1 = rdd6.filter(lambda x: x % 2 == 0)\n",
    "union_rdd_2 = rdd6.filter(lambda x: x % 3 == 0)\n",
    "print(union_rdd_1.union(union_rdd_2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey', 'there', 'This', 'is', 'PySpark', 'RDD', 'Transformations']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatmap_rdd = spark.sparkContext.parallelize([\"Hey there\", \"This is PySpark RDD Transformations\"])\n",
    "(flatmap_rdd.flatMap(lambda x: x.split(\" \")).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hadoop', 'is', 'big', 'data', 'tool']\n"
     ]
    }
   ],
   "source": [
    "rdd4 =  spark.sparkContext.parallelize([\"hadoop is big data tool\"])\n",
    "print(rdd4.flatMap(lambda x: x.split(\" \")).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 2, 3, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([3,4,5]).flatMap(lambda x: range(1,x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 9], [4, 16], [5, 25]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([3,4,5]).map(lambda x: [x,  x*x]).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 4, 16, 5, 25]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize([3,4,5]).flatMap(lambda x: [x, x*x]).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rahul', 48), ('Swati', 45), ('Shreya', 50), ('Abhay', 55), ('Rohan', 44)]\n"
     ]
    }
   ],
   "source": [
    "marks_rdd = spark.sparkContext.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), \\\n",
    "                                            ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "\n",
    "print(marks_rdd.reduceByKey(lambda x, y: x + y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Abhay', 29), ('Abhay', 26), ('Rahul', 25), ('Rahul', 23), ('Rohan', 22), ('Rohan', 22), ('Shreya', 22), ('Shreya', 28), ('Swati', 26), ('Swati', 19)]\n"
     ]
    }
   ],
   "source": [
    "marks_rdd = spark.sparkContext.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), \\\n",
    "                                            ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "\n",
    "print(marks_rdd.sortByKey('ascending').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rahul [25, 23]\n",
      "Swati [26, 19]\n",
      "Shreya [22, 28]\n",
      "Abhay [29, 26]\n",
      "Rohan [22, 22]\n"
     ]
    }
   ],
   "source": [
    "marks_rdd = spark.sparkContext.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), \\\n",
    "                                            ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "dict_rdd = marks_rdd.groupByKey().collect()\n",
    "\n",
    "for key, value in dict_rdd:\n",
    "    print(key, list(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rahul 2\n",
      "Swati 2\n",
      "Rohan 2\n",
      "Shreya 1\n",
      "Abhay 1\n"
     ]
    }
   ],
   "source": [
    "marks_rdd = spark.sparkContext.parallelize([('Rahul', 25), ('Swati', 26), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), \\\n",
    "                                            ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "dict_rdd = marks_rdd.countByKey().items()\n",
    "for key, value in dict_rdd:\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Good', 'Morning'],\n",
       " ['Good', 'Evening'],\n",
       " ['Good', 'Day'],\n",
       " ['Happy', 'Birthday'],\n",
       " ['Happy', 'New', 'Year']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.sparkContext.textFile(r\"C:\\Users\\bmi_cims\\Desktop\\Spark\\data\\greetings.txt\")\n",
    "lines.map(lambda line: line.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'Morning',\n",
       " 'Good',\n",
       " 'Evening',\n",
       " 'Good',\n",
       " 'Day',\n",
       " 'Happy',\n",
       " 'Birthday',\n",
       " 'Happy',\n",
       " 'New',\n",
       " 'Year']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.flatMap(lambda line: line.split()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 23), ('Cat', 11), ('in', 117), ('the', 220), ('Hat', 5)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myFile = spark.sparkContext.textFile(r\"C:\\Users\\bmi_cims\\Desktop\\Spark\\data\\DrSeuss.text\")\n",
    "\n",
    "wordspair = myFile.flatMap(lambda row: row.split(\" \")).map(lambda x: (x, 1)).reduceByKey(lambda x,y : x + y)\n",
    "\n",
    "oldwordcount = wordspair.reduceByKey(lambda x,y : x + y)\n",
    "\n",
    "oldwordcount.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(244, 'the'),\n",
       " (213, 'a'),\n",
       " (203, 'and'),\n",
       " (198, 'i'),\n",
       " (137, 'not'),\n",
       " (126, 'in'),\n",
       " (105, 'to'),\n",
       " (100, 'he'),\n",
       " (99, 'you'),\n",
       " (88, 'like')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myFile = spark.sparkContext.textFile(r\"C:\\Users\\bmi_cims\\Desktop\\Spark\\data\\DrSeuss.text\")\n",
    "\n",
    "wordcounts1 = myFile.map(lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\n",
    "\n",
    "wordcounts2 = wordcounts1.flatMap(lambda x: x.split())\n",
    "\n",
    "wordcounts3 = wordcounts2.map(lambda x: (x, 1))\n",
    "\n",
    "wordcounts4 = wordcounts3.reduceByKey(lambda x,y:x+y)\n",
    "\n",
    "wordcounts5 = wordcounts4.map(lambda x:(x[1],x[0]))\n",
    "\n",
    "wordcounts6 = wordcounts5.sortByKey(ascending=False)\n",
    "\n",
    "# wordcounts6.collect()\n",
    "wordcounts6.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 'f')\n",
      "(1, 'a')\n"
     ]
    }
   ],
   "source": [
    "tsk = spark.sparkContext.parallelize([(1,\"a\"), (2,\"b\"),(1,\"c\"),(2,\"d\"),(1,\"e\"),(3,\"f\")],3)\n",
    "\n",
    "axstream1 = tsk.reduce(max)\n",
    "print(axstream1)\n",
    "\n",
    "axstream2 = tsk.reduce(min)\n",
    "print(axstream2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Write DataFrmes from ExternalFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe from Read external files\n",
    "\n",
    "\n",
    "################################################## Read ####################################################################\n",
    "\n",
    "# Creating df from csv file\n",
    "df2 = spark.read.option(\"header\",True).csv(\"/src/resources/file.csv\")\n",
    "\n",
    "# Creating df from text (TXT) file\n",
    "df2 = spark.read.option(\"header\",True).text(\"/src/resources/file.txt\")\n",
    "\n",
    "# Creating df from JSON file\n",
    "df2 = spark.read.option(\"header\",True).json(\"/src/resources/file.json\")\n",
    "\n",
    "# Creating df from parquet file\n",
    "df2=spark.read.parquet(\"/temp/out/people.parquet\")\n",
    "\n",
    "################################################## Write ####################################################################\n",
    "\n",
    "# Write DataFrame to Externals files\n",
    "\n",
    "# CSV\n",
    "df.write.csv('dataset.csv')\n",
    "\n",
    "# JSON\n",
    "data.write.save('dataset.json', format='json')\n",
    "\n",
    "# Parquet\n",
    "data.write.save('dataset.parquet', format='parquet')\n",
    "\n",
    "# write data to parquest\n",
    "df.write.parquet(\"/tmp/output/people.parquet\")\n",
    "\n",
    "# Append or Overwrite an existing Parquet file\n",
    "df.write.mode('append').parquet(\"/tmp/output/people.parquet\")\n",
    "df.write.mode('overwrite').parquet(\"/tmp/output/people.parquet\")\n",
    "\n",
    "# Create Parquet partition file\n",
    "df.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(\"/tmp/output/people2.parquet\")\n",
    "\n",
    "## Writing selected data to different file formats\n",
    "\n",
    "# CSV\n",
    "data.select(['data', 'open', 'close', 'adjusted']).write.csv('dataset.csv')\n",
    "\n",
    "# JSON\n",
    "data.select(['data', 'open', 'close', 'adjusted']).write.save('dataset.json', format='json')\n",
    "\n",
    "# Parquet\n",
    "data.select(['data', 'open', 'close', 'adjusted']).write.save('dataset.parquet', format='parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame\n",
    "df=spark.createDataFrame(data,columns)\n",
    "\n",
    "# creates a DataFrame with one column id\n",
    "df = spark.range(10) \n",
    "\n",
    "# creates a DataFrame\n",
    "df = spark.sql(\"show tables\")\n",
    "\n",
    "# printSchema\n",
    "df.printSchema()\n",
    "\n",
    "# Checking if a Column Exists in a DataFrame\n",
    "df.schema.fieldNames.contains(\"firstname\")\n",
    "df.schema.contains(StructField(\"firstname\",StringType,true))\n",
    "\n",
    "# CHeck the data types of the column\n",
    "df.select(\"petal_width\").dtypes\n",
    "\n",
    "# chnage the datatype of column in Dataframe\n",
    "df = df.withColumn(\"Quantity\",df.Quantity.cast('int'))\n",
    "df = df.withColumn(\"Calories\",df['calories'].cast(\"Integer\"))\n",
    "\n",
    "# Convert Spark Datafrme to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Display 2 rows and full column contents\n",
    "df.show(2,truncate=False) \n",
    "\n",
    "# Display 2 rows & column values 25 characters\n",
    "df.show(2,truncate=25) \n",
    "\n",
    "# Display DataFrame rows & columns vertically\n",
    "df.show(n=3,truncate=25,vertical=True)\n",
    "\n",
    "# Get the rows in the dataframe as list of row\n",
    "df.take(2)\n",
    "\n",
    "# Change the value of Existing Column\n",
    "from pyspark.sql.functions import col\n",
    "df_value = df.withColumn(\"petal_length\",col(\"petal_length\") * 10)\n",
    "\n",
    "# Rename the Column of DataFrame\n",
    "re_df = df.withColumnRenamed(\"petal_width\",\"petal_width_1\")\n",
    "\n",
    "# Add new column with constant value using 'lit'\n",
    "from pyspark.sql.functions import lit\n",
    "new_col = df.withColumn(\"COllege\",lit(\"MITRC\"))\n",
    "\n",
    "#Drop column\n",
    "drop_df = df.drop(\"COllege\")\n",
    "\n",
    "# greatest\n",
    "from pyspark.sql.functions import greatest,col\n",
    "df1=df.withColumn(\"large\",greatest(col(\"level1\"),col(\"level2\"),col(\"level3\"),col(\"level4\")))\n",
    "\n",
    "# least\n",
    "from pyspark.sql.functions import least,col\n",
    "df2=df.withColumn(\"Small\",least(col(\"level1\"),col(\"level2\"),col(\"level3\"),col(\"level4\")))\n",
    "\n",
    "# Capitalize the first letter of the user name\n",
    "from pyspark.sql.functions import initcap\n",
    "df = df.withColumn('name_cap', initcap('user_name'))\n",
    "\n",
    "\n",
    "############################################### Missing and Fill values #######################################################\n",
    "\n",
    "# Drop Duplicates\n",
    "df = df.dropDuplicates(['user_id']).orderBy('age')\n",
    "\n",
    "# Replace Null/None Value with Empty String\n",
    "df.na.fill(\"\").show(false)\n",
    "\n",
    "#Replace 0 for null for all integer columns\n",
    "df.na.fill(value=0).show()\n",
    "\n",
    "#Replace Replace 0 for null on only population column \n",
    "df.na.fill(value=0,subset=[\"population\"]).show()\n",
    "\n",
    "df.na.fill(\"unknown\",[\"city\"]).na.fill(\"\",[\"type\"]).show()\n",
    "\n",
    "df.na.fill({\"city\": \"unknown\", \"type\": \"\"}).show()\n",
    "\n",
    "##################################################### Select ##################################################################\n",
    "# Show specific column data\n",
    "df.select(\"firstname\").show()\n",
    "df.select(\"firstname\",\"lastname\").show()\n",
    "\n",
    "# Select All columns from List\n",
    "columns = ['firstname','lastname','country','state']\n",
    "df.select(*columns).show()\n",
    "\n",
    "df.select([col for col in df.columns]).show()\n",
    "df.select(\"*\").show()\n",
    "\n",
    "# Select Columns by Index\n",
    "df.select(df.columns[:3]).show(3)\n",
    "\n",
    "#Selects columns 2 to 4  and top 3 rows\n",
    "df.select(df.columns[2:4]).show(3)\n",
    "\n",
    "\n",
    "##################################################### Aggregation ##############################################################\n",
    "\n",
    "# Average\n",
    "from pyspark.sql.functions import avg\n",
    "df_avg.select(avg(\"Total Cost\").alias(\"Average Cost\")).show()\n",
    "\n",
    "# sum\n",
    "from pyspark.sql.functions import sum\n",
    "df_avg.select(sum(\"Quantity\").alias(\"Total Items\")).show()\n",
    "\n",
    "# groupBy\n",
    "df.groupBy(\"Shop_Name\").sum(\"Quantity\").show()\n",
    "\n",
    "#Average money earn\n",
    "df_avg.groupBy(\"Shop_Name\").avg(\"Total Cost\").show()\n",
    "\n",
    "# max\n",
    "from pyspark.sql.functions import max\n",
    "df.select(max(\"Quantity\").alias(\"Maximum Quantity\")).show()\n",
    "\n",
    "# min\n",
    "from pyspark.sql.functions import min\n",
    "df.groupBy(\"Shop_Name\").min(\"Quantity\").show()\n",
    "\n",
    "# count\n",
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"Quantity\")).show()\n",
    "\n",
    "# distinct\n",
    "df.select(df[\"Shop_Name\"]).distinct().show()\n",
    "\n",
    "# collection list\n",
    "from pyspark.sql.functions import collect_list\n",
    "df.select(collect_list(\"Shop_Name\")).show(truncate=False)\n",
    "\n",
    "# set\n",
    "from pyspark.sql.functions import collect_set\n",
    "df.select(collect_set(\"Shop_Name\")).show(truncate=False)\n",
    "\n",
    "# countDistinct\n",
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"Shop_Name\")).show()\n",
    "\n",
    "# Kurthosis\n",
    "from pyspark.sql.functions import kurtosis\n",
    "df.select(kurtosis('Quantity')).show(truncate=False)\n",
    "\n",
    "# mean\n",
    "from pyspark.sql.functions import mean\n",
    "df.select(mean('Quantity')).show(truncate=False)\n",
    "\n",
    "# skewness function\n",
    "from pyspark.sql.functions import skewness\n",
    "df.select(skewness('Quantity')).show(truncate=False)\n",
    "\n",
    "# stddev\n",
    "from pyspark.sql.functions import stddev,stddev_samp,stddev_pop\n",
    "df.select(stddev(\"Quantity\"), stddev_samp(\"Quantity\"),stddev_pop(\"Quantity\")).show(truncate=False)\n",
    "\n",
    "# sumDistinct\n",
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show(truncate=False)\n",
    "\n",
    "# Variance\n",
    "from pyspark.sql.functions import variance,var_samp,var_pop\n",
    "df.select(variance(\"Quantity\"),var_samp(\"Quantity\"),var_pop(\"Quantity\")).show(truncate=False)\n",
    "\n",
    "\n",
    "##################################################### Filters ################################################################\n",
    "\n",
    "# Like SQL Expression\n",
    "from pyspark.sql.functions import filter\n",
    "from pyspark.sql.functions import col\n",
    "df.filter(col(\"Dept No\") ==1).show()\n",
    "\n",
    "\n",
    "# Filtering with multiple condition AND(&&);NOT(!) OR(||)\n",
    "from pyspark.sql.functions import filter\n",
    "df.filter((df[\"Gender\"]==\"M\") & (df[\"Dept No\"]==2)).show()\n",
    "\n",
    "# Filtering on an array column\n",
    "from pyspark.sql.functions import filter\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.filter(array_contains(df[\"Languages\"],\"Python\")).show()\n",
    "\n",
    "\n",
    "#filter data by null values\n",
    "from pyspark.sql.functions import filter\n",
    "from pyspark.sql.functions import isNotNull\n",
    "df.filter(df.name.isNotNull()).show()\n",
    "\n",
    "from pyspark.sql.functions import filter\n",
    "from pyspark.sql.functions import isNull\n",
    "df.filter(df.name.isNull()).show()\n",
    "\n",
    "##################################################### When ################################################################\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "df.select(\"name\", when(df.vitamins >= \"25\", \"rich in vitamins\")).show()\n",
    "\n",
    "from pyspark.sql.functions import col,when\n",
    "df_when  = df.withColumn('Gender',when(col(\"Gender\") == \"M\",\"Male\").when(col(\"Gender\") == \"F\",\"Female\").otherwise(\"Other\"))\n",
    "df_when.show()\n",
    "\n",
    "df_when2  = df.select(col(\"*\"),when(col(\"Gender\") == \"M\",\"Male\").when(col(\"Gender\") == \"F\",\"Female\").otherwise(\"Unknow\")\n",
    "                                                                                                    .alias(\"New_gender\"))\n",
    "\n",
    "################################################ Sort and OrderBy #############################################################\n",
    "# Sort the dataframe\n",
    "df.sort(\"department\",\"state\").show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df.sort(col(\"department\"),col(\"state\")).show(truncate=False)\n",
    "\n",
    "df.orderBy(\"department\",\"state\").show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df.orderBy(col(\"department\"),col(\"state\")).show(truncate=False)\n",
    "\n",
    "# Sort by Ascending (ASC)\n",
    "df.sort(df.department.asc(),df.state.asc()).show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df.sort(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n",
    "\n",
    "# Sort by Descending (DESC)\n",
    "df.sort(df.department.asc(),df.state.desc()).show(truncate=False)\n",
    "\n",
    "df.sort(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\n",
    "\n",
    "df.orderBy(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\n",
    "\n",
    "################################################ Window function #############################################################\n",
    "\n",
    "# row_number \n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)).show(truncate=False)\n",
    "\n",
    "# rank\n",
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)).show()\n",
    "\n",
    "# dense_rank\n",
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)).show()\n",
    "\n",
    "# percent_rank\n",
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)).show()\n",
    "\n",
    "# ntile\n",
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)).show()\n",
    "\n",
    "# cume_dist\n",
    "from pyspark.sql.functions import cume_dist    \n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)).show()\n",
    "\n",
    "# lag\n",
    "from pyspark.sql.functions import lag    \n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)).show()\n",
    "\n",
    "# lead\n",
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)).show()\n",
    "\n",
    "\n",
    "####################################################### Joins ################################################################\n",
    "\n",
    "# Employee dataframe\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "\n",
    "# Department dataFrame\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "\n",
    "# Inner joins\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\").show(truncate=False)\n",
    "\n",
    "# Full Outer Join\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\").show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\").show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\").show(truncate=False)\n",
    "\n",
    "# Left Outer Join\n",
    "empDF.join(deptDF,empDF(\"emp_dept_id\") ==  deptDF(\"dept_id\"),\"left\").show(false)\n",
    "empDF.join(deptDF,empDF(\"emp_dept_id\") ==  deptDF(\"dept_id\"),\"leftouter\").show(false)\n",
    "\n",
    "# Right Outer Join\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\").show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\").show(truncate=False)\n",
    "\n",
    "# Left Semi Join\n",
    "\n",
    "# Left Anti Join\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\").show(truncate=False)\n",
    "\n",
    "####################################################### Union ################################################################\n",
    "\n",
    "df1 = spark.createDataFrame(data = simpleData1, schema = columns1)\n",
    "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
    "\n",
    "unionDF = df1.union(df2)\n",
    "unionDF.show(truncate=False)\n",
    "\n",
    "# Merge without Duplicates\n",
    "disDF = df.union(df2).distinct()\n",
    "disDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Expresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing SQL Expression\n",
    "df.createOrReplaceTempView(\"parquetTable\")\n",
    "parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")\n",
    "\n",
    "# using parquest path\n",
    "spark.sql(\"CREATE TEMPORARY VIEW PERSON USING parquet OPTIONS (path \\\"/tmp/output/people.parquet\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON\").show()\n",
    "\n",
    "#  Change Column type using SQL Expression\n",
    "df.createOrReplaceTempView(\"Table\")\n",
    "df_sql = spark.sql(\"SELECT STRING(species),Float(sepal_length) from Table\")\n",
    "df_sql.printSchema()\n",
    "\n",
    "df_sql.show(5)\n",
    "\n",
    "# Change Column type using selectExpr\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df_new = df.selectExpr(\"cast(petal_width as Double ) petal_width\", \"cast(sepal_length as Double) sepal_length\")\n",
    "df_new.printSchema()\n",
    "df_new.show(5)\n",
    "\n",
    "# case when\n",
    "\n",
    "# Question: Create a column “Performance” and find it out on the basis of percentage?\n",
    "from pyspark.sql.functions import expr\n",
    "df_case = df.withColumn(\"Performance\", expr(\"case when Percentage>88.0 then 'Excellent' \" \\ \n",
    "                                            + \"when Percentage<83.0 then 'Average' \"  \\\n",
    "                                            + \"else 'Great' end\"))\n",
    "# Order by \n",
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\").show(truncate=False)\n",
    "\n",
    "# Joins\n",
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")\n",
    "\n",
    "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\").show(truncate=False)\n",
    "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
