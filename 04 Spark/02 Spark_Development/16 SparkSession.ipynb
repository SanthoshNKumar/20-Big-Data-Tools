{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    - History\n",
    "    - Create SparkSession\n",
    "    - Create Sample DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "History\n",
    "\n",
    "- In Spark 1.x, three entry points were introduced:\n",
    "    - SparkContext, SQLContext and HiveContext.\n",
    "    \n",
    "- Since Spark 2.x, a new entry point called 'SparkSession'.\n",
    "- Spark 2.0 introduced a new entry point called SparkSession that essentially replaced both SQLContext and HiveContext. \n",
    "\n",
    "Spark Session\n",
    "    - SparkSession introduced in version Spark 2.0.\n",
    "    - Spark session is a combination of all these different contexts\n",
    "    - It is an entry point to underlying Spark functionality in order to programmatically create Spark RDD, DataFrame and \n",
    "      DataSet.\n",
    "    - It unifies all the different contexts in spark and avoids the developer to worry about creating difference contexts.\n",
    "    - Instead of having a spark context, hive context, SQL context, now all of it is encapsulated in a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spark Session also includes all the APIs available in different contexts\n",
    "\n",
    "    Spark Context,\n",
    "    SQL Context,\n",
    "    Streaming Context,\n",
    "    Hive Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Be default 'Spark shell' provides “spark” object which is an instance of SparkSession class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SparkSession commonly used methods\n",
    "\n",
    "    - builder() – builder() is used to create a new SparkSession, this return SparkSession.Builder\n",
    "    \n",
    "    - createDataFrame() – This creates a DataFrame from a collection and an RDD\n",
    "\n",
    "    - createDataset() – This creates a Dataset from the collection, DataFrame, and RDD.\n",
    "    \n",
    "    - emptyDataFrame() – Creates an empty DataFrame.\n",
    "    \n",
    "    - emptyDataset() – Creates an empty Dataset.\n",
    "    \n",
    "    - getActiveSession() – returns an active Spark session\n",
    "    \n",
    "    - implicits() – You can access the nested Scala object.\n",
    "    \n",
    "    - read() – Returns an instance of DataFrameReader class, this is used to read records from csv, parquet, avro and more \n",
    "      file formats into DataFrame.\n",
    "      \n",
    "    - readStream() – Returns an instance of DataStreamReader class, this is used to read streaming data. that can be used to \n",
    "      read streaming data into DataFrame.\n",
    "    \n",
    "    - sparkContext() – Returns a SparkContext.\n",
    "    \n",
    "    - sql – Returns a DataFrame after executing the SQL mentioned.\n",
    "    \n",
    "    - sqlContext() – Returns SQLContext.\n",
    "\n",
    "    - stop() – Stop the current SparkContext.\n",
    "\n",
    "    - table() – Returns a DataFrame of a table or view.\n",
    "    \n",
    "    - udf() – Creates a Spark UDF to use it on DataFrame, Dataset and SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Win10.eu2d4xqpkiwuzc50gvspb3qxbb.bx.internal.cloudapp.net:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkByExamples.com</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x22bf9216fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing pyspark Intallation\n",
    "\n",
    "import findspark\n",
    "findspark.init('C:\\Spark')\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.3\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First SparkContext:\n",
      "APP Name :SparkByExamples.com\n",
      "Master :local[1]\n"
     ]
    }
   ],
   "source": [
    "print(\"First SparkContext:\")\n",
    "print(\"APP Name :\"+spark.sparkContext.appName);\n",
    "print(\"Master :\"+spark.sparkContext.master);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second SparkContext:\n",
      "APP Name :SparkByExamples.com\n",
      "Master :local[1]\n"
     ]
    }
   ],
   "source": [
    "sparkSession2 = SparkSession.builder.master(\"local[1]\").appName(\"SparkByExample-test\").getOrCreate();\n",
    "\n",
    "print(\"Second SparkContext:\")\n",
    "print(\"APP Name :\"+sparkSession2.sparkContext.appName);\n",
    "print(\"Master :\"+sparkSession2.sparkContext.master);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame from SparkSession\n",
    "\n",
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Using toDF() function\n",
    "dfFromRDD1 = rdd.toDF()\n",
    "\n",
    "# Column names to the DataFrame use toDF() method with column\n",
    "dfFromRDD1 = rdd.toDF(columns)\n",
    "dfFromRDD1.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
