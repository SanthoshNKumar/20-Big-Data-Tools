{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** Note : Start the Spark maste and Slave before running below the code\n",
    "\n",
    "    # Start master\n",
    "        # C:\\Spark\\bin>spark-class org.apache.spark.deploy.master.Master\n",
    "\n",
    "    # start Slave\n",
    "        # C:\\Spark\\bin>spark-class2.cmd org.apache.spark.deploy.worker.Worker -c 1 -m 4G spark://10.0.0.4:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bmi_cims\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\bmi_cims\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "c:\\users\\bmi_cims\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Win10.eu2d4xqpkiwuzc50gvspb3qxbb.bx.internal.cloudapp.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkByExamples.com</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e4631964e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('C:\\Spark')\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame from SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "# Create RDD\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Using toDF() function\n",
    "df = rdd.toDF()\n",
    "\n",
    "# Column names to the DataFrame use toDF() method with column\n",
    "df = rdd.toDF(columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Writing entire data to different file formats\n",
    "\n",
    "# CSV\n",
    "df.write.csv('dataset.csv')\n",
    "\n",
    "# JSON\n",
    "data.write.save('dataset.json', format='json')\n",
    "\n",
    "# Parquet\n",
    "data.write.save('dataset.parquet', format='parquet')\n",
    "\n",
    "## Writing selected data to different file formats\n",
    "\n",
    "# CSV\n",
    "data.select(['data', 'open', 'close', 'adjusted']).write.csv('dataset.csv')\n",
    "\n",
    "# JSON\n",
    "data.select(['data', 'open', 'close', 'adjusted']).write.save('dataset.json', format='json')\n",
    "\n",
    "# Parquet\n",
    "data.select(['data', 'open', 'close', 'adjusted']).write.save('dataset.parquet', format='parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+\n",
      "|         _c0|        _c1|         _c2|        _c3|        _c4|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|    species|\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|          3|         1.4|        0.2|Iris-setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'data/IRIS.csv'\n",
    "\n",
    "df = spark.read.csv(csv_file)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|    species|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|          3|         1.4|        0.2|Iris-setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
      "|           5|        3.6|         1.4|        0.2|Iris-setosa|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV File\n",
    "df = spark.read.option(\"header\",True).csv(\"data/IRIS.csv\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame with schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "# Define Data\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),(\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "         (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),(\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "         (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "\n",
    "# Define the Schema\n",
    "schema = StructType([StructField(\"firstname\",StringType(),True),StructField(\"middlename\",StringType(),True),\n",
    "                     StructField(\"lastname\",StringType(),True),StructField(\"id\", StringType(), True), \n",
    "                     StructField(\"gender\", StringType(), True),StructField(\"salary\", IntegerType(), True)])\n",
    "\n",
    "# Create DataFrame from Schema\n",
    "df = spark.createDataFrame(data=data2,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert PySpark Dataframe to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  firstname middlename  lastname     id gender  salary\n",
      "0     James                Smith  36636      M    3000\n",
      "1   Michael       Rose            40288      M    4000\n",
      "2    Robert             Williams  42114      M    4000\n",
      "3     Maria       Anne     Jones  39192      F    4000\n",
      "4       Jen       Mary     Brown             F      -1\n"
     ]
    }
   ],
   "source": [
    "#  PySpark DataFrame can be converted to Python Pandas DataFrame using a function toPandas()\n",
    "\n",
    "pandasDF = df.toPandas()\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "\n",
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]})\n",
    "\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame from Data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4.1 Creating DataFrame from CSV\n",
    "df2 = spark.read.csv(\"/src/resources/file.csv\")\n",
    "\n",
    "#4.2 Creating from text (TXT) file\n",
    "df2 = spark.read.text(\"/src/resources/file.txt\")\n",
    "\n",
    "#4.3 Creating from JSON file\n",
    "df2 = spark.read.json(\"/src/resources/file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other sources (Avro, Parquet, ORC, Kafka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 PySpark Read and Write Parquet File\n",
    "\n",
    "df.write.parquet(\"/tmp/out/people.parquet\") \n",
    "parDF1=spark.read.parquet(\"/temp/out/people.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Empty RDD in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmptyRDD[33] at emptyRDD at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "#Creates Empty RDD\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "print(emptyRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmptyRDD[54] at emptyRDD at NativeMethodAccessorImpl.java:0\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n",
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creates Empty RDD\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "print(emptyRDD)\n",
    "\n",
    "# Create Empty DataFrame with Schema (StructType)\n",
    "\n",
    "#Create Schema\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "schema = StructType([StructField('firstname', StringType(), True),StructField('middlename', StringType(), True),\n",
    "                     StructField('lastname', StringType(), True)])\n",
    "\n",
    "#Create empty DataFrame from empty RDD\n",
    "df = spark.createDataFrame(emptyRDD,schema)\n",
    "df.printSchema()\n",
    "\n",
    "# Convert Empty RDD to DataFrame\n",
    "df1 = emptyRDD.toDF(schema)\n",
    "df1.printSchema()\n",
    "\n",
    "# Create Empty DataFrame with Schema.\n",
    "df2 = spark.createDataFrame([], schema)\n",
    "df2.printSchema()\n",
    "\n",
    "# Create Empty DataFrame without Schema (no columns)\n",
    "df3 = spark.createDataFrame([], StructType([]))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Spark Nested Struct DataFrame to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n",
      "                   name    dob gender salary\n",
      "0      (James, , Smith)  36636      M   3000\n",
      "1     (Michael, Rose, )  40288      M   4000\n",
      "2  (Robert, , Williams)  42114      M   4000\n",
      "3  (Maria, Anne, Jones)  39192      F   4000\n",
      "4    (Jen, Mary, Brown)             F     -1\n"
     ]
    }
   ],
   "source": [
    "# Nested structure elements\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
    "dataStruct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"),((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"),\n",
    "              ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"),((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"),\n",
    "              ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\")]\n",
    "\n",
    "schemaStruct = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "          StructField('dob', StringType(), True),\n",
    "          StructField('gender', StringType(), True),\n",
    "          StructField('salary', StringType(), True)\n",
    "         ])\n",
    "\n",
    "df = spark.createDataFrame(data=dataStruct, schema = schemaStruct)\n",
    "df.printSchema()\n",
    "\n",
    "pandasDF2 = df.toPandas()\n",
    "print(pandasDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark Write DataFrame to Parquet file format\n",
    "\n",
    "data =[(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),(\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "       (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),(\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "       (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "       \n",
    "columns=[\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "df=spark.createDataFrame(data,columns)\n",
    "\n",
    "df.write.parquet(\"/tmp/output/people.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark Read Parquet file into DataFrame\n",
    "\n",
    "parDF=spark.read.parquet(\"/tmp/output/people.parquet\")\n",
    "\n",
    "# Append or Overwrite an existing Parquet file\n",
    "df.write.mode('append').parquet(\"/tmp/output/people.parquet\")\n",
    "df.write.mode('overwrite').parquet(\"/tmp/output/people.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing SQL queries DataFrame\n",
    "\n",
    "parqDF.createOrReplaceTempView(\"ParquetTable\")\n",
    "parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")\n",
    "\n",
    "# Creating a table on Parquet file\n",
    "\n",
    "spark.sql(\"CREATE TEMPORARY VIEW PERSON USING parquet OPTIONS (path \\\"/tmp/output/people.parquet\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON\").show()\n",
    "\n",
    "# Create Parquet partition file\n",
    "df.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(\"/tmp/output/people2.parquet\")\n",
    "\n",
    "# Retrieving from a partitioned Parquet file\n",
    "parDF2=spark.read.parquet(\"/tmp/output/people2.parquet/gender=M\")\n",
    "parDF2.show(truncate=False)\n",
    "\n",
    "# Creating a table on Partitioned Parquet file\n",
    "spark.sql(\"CREATE TEMPORARY VIEW PERSON2 USING parquet OPTIONS (path \\\"/tmp/output/people2.parquet/gender=F\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON2\" ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Example of PySpark read and write Parquet file\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"parquetFile\").getOrCreate()\n",
    "data =[(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),(\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "       (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),(\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "       (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "\n",
    "columns=[\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.write.mode(\"overwrite\").parquet(\"/tmp/output/people.parquet\")\n",
    "parDF1=spark.read.parquet(\"/tmp/output/people.parquet\")\n",
    "parDF1.createOrReplaceTempView(\"parquetTable\")\n",
    "parDF1.printSchema()\n",
    "parDF1.show(truncate=False)\n",
    "\n",
    "parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")\n",
    "parkSQL.show(truncate=False)\n",
    "\n",
    "spark.sql(\"CREATE TEMPORARY VIEW PERSON USING parquet OPTIONS (path \\\"/tmp/output/people.parquet\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON\").show()\n",
    "\n",
    "df.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(\"/tmp/output/people2.parquet\")\n",
    "\n",
    "parDF2=spark.read.parquet(\"/tmp/output/people2.parquet/gender=M\")\n",
    "parDF2.show(truncate=False)\n",
    "\n",
    "spark.sql(\"CREATE TEMPORARY VIEW PERSON2 USING parquet OPTIONS (path \\\"/tmp/output/people2.parquet/gender=F\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON2\" ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
