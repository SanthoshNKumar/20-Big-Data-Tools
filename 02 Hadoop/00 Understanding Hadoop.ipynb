{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "File System :\n",
    "\n",
    "Verticle Scaling : \n",
    "    - When we are scaling(Increasing) capacity(RAM/Storage) of the single system is called Verticle Scaling.\n",
    "\n",
    "Horizontal Scaling : \n",
    "    - Adding more processing units or physical machines to your server or data base.It involves growing the number of nodes in \n",
    "      the cluster(Computer Network).\n",
    "      \n",
    "Latency : \n",
    "    - Time to get the first record( Minimize this lower is better)\n",
    "\n",
    "Throughput : \n",
    "    - Number of records processed per unit of time\n",
    "\n",
    "Computer Network : \n",
    "    - It is a group of computers that uses a set of common communication protocols over digital interconnections \n",
    "      for the purpose of sharing resources located on or provided by the network nodes.\n",
    "\n",
    "Rack : its a kind of box where we fix multiple computers into it. each rack given indivisual power suppy and netwrok switch.\n",
    "\n",
    "Blocks : HDFS splits large files into small chunks known as blocks.\n",
    "\n",
    "Parallel computing :\n",
    "    - Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously\n",
    "    - Large problems can often be divided into smaller ones, which can then be solved at the same time\n",
    "    \n",
    "Multiprocessing :\n",
    "    - Multiprocessing is the use of two or more central processing units within a single computer system\n",
    "\n",
    "Hadoop Ecosystem\n",
    "    - Hadoop Ecosystem is a platform or a suite which provides various services to solve the big data problems.\n",
    "\n",
    "HDFS : Hadoop Distributed File System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "HDFS Features :\n",
    "\n",
    "    - Distributed\n",
    "        - Hadoop stores data in such a way that insted of relaying on single large machine it uses network of several machines \n",
    "          when you consume combined storage capcity of your network buy few more cheper computers and add them into the \n",
    "          cluster(Horizontal Scaling)\n",
    "        - We need a software that combines the storage capcity of entire network into a single unit. as a uses we wanted view \n",
    "          as single lateg Disk\n",
    "        - HDFS provides these facility we dont have to wory about how HDFS storages data on indivisual computer.\n",
    "        \n",
    "    - Scalable\n",
    "        - Its is scalabel\n",
    "        - When we need more storage buy more computers and add them into cluster (computer netwrok)\n",
    "        - HDFS is horizontally scalable it will never run out of storage\n",
    "        \n",
    "    - Cost Effective\n",
    "        - Here we can start as a small as single computer and scale it as an when it required.\n",
    "        - We dont need to buy high end server machines. we can get inexpensive commodity hardware, thus reduces storage costs\n",
    "        \n",
    "    - Fault Tolerant\n",
    "        - When we create net work using 100 of commodity machine. it is likely that somethng breaks every month or weeks some \n",
    "          computer crsahes/netwrok switch fails/Disk fails anything can happen \n",
    "          HDFS is capable of tolerating such failures so even if drives crsshes your HDFS able to works without any \n",
    "          probelms.system able to work and not loose the data\n",
    "          \n",
    "    - High Throughput\n",
    "        - Hadoop HDFS stores data in a distributed fashion, which allows data to be processed parallelly on a cluster of nodes.\n",
    "          This decreases the processing time and thus provides high throughput.\n",
    "          \n",
    "    - High Availability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hadoop Cluster : HDFS have master slave architecture\n",
    "    - HDFS Cluster consists of single Name node(Master) and one or more Data nodes(Slaves)\n",
    "    - Name Nodes manages File system namespace\n",
    "    - All the Clinet Interaction starts with Name Nodes\n",
    "    - Data node stores files data as Blocks\n",
    "    - Data node intercats with NM on block report\n",
    "    - File is broken into blocks and stored on data node\n",
    "    - Name node maintains file to block mapping, location, order of blocks and other metadata\n",
    "    - Default block size is 128 MB\n",
    "    - You can change the block size for a file\n",
    "    - Name node determines the mapping of blocks into data node but after mapping. Client directly interacts with data node \n",
    "      for reading/ writing blocks\n",
    "    - Name node and Data node can be installed on single machine to create a single node cluster for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Replication factor : \n",
    "\n",
    "fsImage : \n",
    "\n",
    "Edit log : Name node maintenance an editlog in its local disk every time name node makes changes on the file system ir \n",
    "roecords that change in the edit log\n",
    "\n",
    "Backup in HDFS :\n",
    "    - HDFS namespace information (Name node) : All the information that name node maintenance should be continuosly backedup at \n",
    "      some other place.Take backup of editlog\n",
    "    - Standby Namenode machine : standby preconfigured computer ready to takeover the role of name node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Core Components of Hadoop\n",
    "    - A distributed file System(HDFS)\n",
    "    - A distributed Programming framework(Map Reduce)\n",
    "    - YARN (Yet Another Resource Negotiator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Components that collectively form a Hadoop ecosystem:\n",
    "    - HDFS: Hadoop Distributed File System\n",
    "    - YARN: Yet Another Resource Negotiator\n",
    "    - MapReduce: Programming based Data Processing\n",
    "    - Spark: In-Memory data processing\n",
    "    - PIG, HIVE: Query based processing of data services\n",
    "    - HBase: NoSQL Database\n",
    "    - Mahout, Spark MLLib: Machine Learning algorithm libraries\n",
    "    - Solar, Lucene: Searching and Indexing\n",
    "    - Zookeeper: Managing cluster\n",
    "    - Oozie: Job Scheduling\n",
    "    \n",
    "MapReduce : \n",
    "    - MapReduce is a programming model or pattern within the Hadoop framework that is used to access big data stored in the \n",
    "      Hadoop File System (HDFS).\n",
    "    - It is a framework or a programming model in the  Hadoop ecosystem to process large unstructured data sets in distributed\n",
    "      manner by using large number of nodes.\n",
    "      \n",
    "Pig and Hve: \n",
    "    - Pig and Hive are components that sit on top of Hadoop framework for processing large data sets without the users having \n",
    "      to write Java based MapReduce code.\n",
    "      \n",
    "Drawbacks in MapReduce:\n",
    "    - Hadoop MapReduce allows programmers to filter and aggregate data from HDFS to gain meaningful insights from big data.\n",
    "    -  The Map and Reduce algorithmic functions can also be implemented using C,  Python and Java\n",
    "    -  The only drawback to use the coding approach of Hadoop MapReduce is that hadoop developers need to write several lines \n",
    "       of basic java code requiring extra effort and time for code review and QA. \n",
    "       \n",
    "Streaming data\n",
    "    - Streaming data is data that is continuously generated by different sources.\n",
    "\n",
    "Batch Data:\n",
    "    - Data is where a group of transactions is collected over a period of time. \n",
    "\n",
    "RDD : Resilient Distributed Dataset\n",
    "\n",
    "YAN : (Yet Another Resource Negotiator) : Cluster Resource Management and Task execution.\n",
    "\n",
    "\n",
    "Driver Program : \n",
    "    - A Driver Program is where your writes Spark code using either Scala,Java,Python. \n",
    "    - It is Responsible for launching various parallel operation on the cluster.\n",
    "\n",
    "Executor : \n",
    "    - Its is a JVM (Java Virual Machine) thats runs on the worker node of the cluster.\n",
    "    - It Provides hardware resources for running the tasks launched by the driver program.\n",
    "\n",
    "As soon as Spark job sumitted,the driver program launches various operation on each executors.driver and executor together make \n",
    "an application\n",
    "\n",
    "Libraries Available in Spark\n",
    "    - Batch Analysis\n",
    "    - Streaming\n",
    "    - Machine Learning\n",
    "    - Graph Analysis\n",
    "\n",
    "Pig and Hive:\n",
    "    - Pig hadoop and Hive hadoop have a similar goal- they are tools that ease the complexity of writing complex java MapReduce \n",
    "      programs.\n",
    "\n",
    "Hive : \n",
    "    - Hive is an application that runs over the Hadoop framework and provides SQL like interface for processing/query the data.\n",
    "      Hive enables SQL developers to write Hive Query Language (HQL) statements that are similar to standard SQL statements for\n",
    "      data query and analysis.\n",
    "\n",
    "Pig :\n",
    "    - Pig is a high-level platform or tool which is used to process the large datasets. \n",
    "    - It provides a high-level of abstraction for processing over the MapReduce.\n",
    "    - It provides a high-level scripting language, known as Pig Latin which is used to develop the data analysis codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hadoop Distribution Vendoers : \n",
    "    - Amazon Elastic MapReduce\n",
    "    - Cloudera CDH Hadoop Distribution\n",
    "    - Hortonworks Data Platform (HDP)\n",
    "    - MapR Hadoop Distribution\n",
    "    - IBM Open Platform\n",
    "    - Microsoft Azure's HDInsight -Cloud based Hadoop Distrbution\n",
    "    - Dell- Cloudera Apache Hadoop Solution.\n",
    "    - Datastax Enterprise Analytics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
